{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "## Purpose\n",
    "This notebook handles the collection of raw movie data from multiple sources including:\n",
    "- **TMDB API**: Movie metadata (budget, cast, crew, genres, runtime, release dates)\n",
    "- **Box Office Mojo**: Box office revenue data (opening weekend, total domestic, worldwide)\n",
    "- **OMDb API**: Supplemental metadata and IMDb ratings\n",
    "- **YouTube Data API**: Trailer view counts and engagement metrics\n",
    "\n",
    "## Objectives\n",
    "1. Set up API connections and test endpoints\n",
    "2. Write data collection functions with error handling and rate limiting\n",
    "3. Collect data for 3,000+ movies from 2010-2024\n",
    "4. Merge data sources on IMDb ID\n",
    "5. Save raw datasets to CSV files in `data/raw/` directory\n",
    "6. Perform initial data inspection\n",
    "\n",
    "## Outputs\n",
    "- `data/raw/movies_tmdb_raw.csv`\n",
    "- `data/raw/revenue_boxofficemojo_raw.csv`\n",
    "- `data/raw/trailers_youtube_raw.csv`\n",
    "\n",
    "## Notes\n",
    "- This notebook may take several hours to run due to API rate limits\n",
    "- Once data is collected, subsequent runs should load from saved CSV files\n",
    "- API keys should be stored in a `.env` file (not committed to git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys loaded:\n",
      "  TMDB: ✓\n",
      "  OMDb: ✓\n",
      "  YouTube: ✓\n",
      "\n",
      "Testing TMDB API connection...\n",
      "✓ TMDB API connection successful!\n",
      "  Test movie: Fight Club\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "TMDB_API_KEY = os.getenv('TMDB_API_KEY')\n",
    "OMDB_API_KEY = os.getenv('OMDB_API_KEY')\n",
    "YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"API Keys loaded:\")\n",
    "print(f\"  TMDB: {'✓' if TMDB_API_KEY else '✗'}\")\n",
    "print(f\"  OMDb: {'✓' if OMDB_API_KEY else '✗'}\")\n",
    "print(f\"  YouTube: {'✓' if YOUTUBE_API_KEY else '✗'}\")\n",
    "\n",
    "# Test TMDB API connection\n",
    "print(\"\\nTesting TMDB API connection...\")\n",
    "test_url = f\"https://api.themoviedb.org/3/movie/550?api_key={TMDB_API_KEY}\"\n",
    "try:\n",
    "    response = requests.get(test_url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ TMDB API connection successful!\")\n",
    "        print(f\"  Test movie: {response.json()['title']}\")\n",
    "    else:\n",
    "        print(f\"✗ TMDB API error: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# TMDB API Base URL\n",
    "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "# Rate limiter class to handle TMDB's 40 requests per 10 seconds limit\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls=40, time_period=10):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_period = time_period\n",
    "        self.calls = []\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Remove calls older than time_period\n",
    "        self.calls = [call_time for call_time in self.calls if now - call_time < self.time_period]\n",
    "        \n",
    "        if len(self.calls) >= self.max_calls:\n",
    "            sleep_time = self.time_period - (now - self.calls[0]) + 0.1\n",
    "            print(f\"  Rate limit reached, waiting {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            self.calls = []\n",
    "        \n",
    "        self.calls.append(time.time())\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(max_calls=35, time_period=10)  # Using 35 to be safe\n",
    "\n",
    "def get_popular_movies_by_year(year, pages=5):\n",
    "    \"\"\"\n",
    "    Get popular movies for a specific year using TMDB discover endpoint.\n",
    "    \n",
    "    Args:\n",
    "        year: Release year (e.g., 2020)\n",
    "        pages: Number of pages to fetch (20 movies per page)\n",
    "    \n",
    "    Returns:\n",
    "        List of movie IDs\n",
    "    \"\"\"\n",
    "    movie_ids = []\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        rate_limiter.wait_if_needed()\n",
    "        \n",
    "        url = f\"{TMDB_BASE_URL}/discover/movie\"\n",
    "        params = {\n",
    "            'api_key': TMDB_API_KEY,\n",
    "            'language': 'en-US',\n",
    "            'sort_by': 'popularity.desc',\n",
    "            'primary_release_year': year,\n",
    "            'page': page,\n",
    "            'vote_count.gte': 50  # Minimum votes to ensure it's not obscure\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                movie_ids.extend([movie['id'] for movie in data['results']])\n",
    "            else:\n",
    "                print(f\"  Error fetching page {page} for year {year}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Exception for year {year}, page {page}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return movie_ids\n",
    "\n",
    "def get_movie_details(movie_id):\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific movie.\n",
    "    \n",
    "    Args:\n",
    "        movie_id: TMDB movie ID\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with movie details or None if error\n",
    "    \"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    \n",
    "    url = f\"{TMDB_BASE_URL}/movie/{movie_id}\"\n",
    "    params = {\n",
    "        'api_key': TMDB_API_KEY,\n",
    "        'append_to_response': 'credits,release_dates,videos'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"  Error fetching movie {movie_id}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Exception for movie {movie_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_movie_data(movie_details):\n",
    "    \"\"\"\n",
    "    Extract relevant fields from TMDB movie details.\n",
    "    \n",
    "    Args:\n",
    "        movie_details: Raw JSON response from TMDB\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extracted fields\n",
    "    \"\"\"\n",
    "    if not movie_details:\n",
    "        return None\n",
    "    \n",
    "    # Extract release dates to find US release\n",
    "    us_release_date = None\n",
    "    us_certification = None\n",
    "    if 'release_dates' in movie_details and 'results' in movie_details['release_dates']:\n",
    "        for country_release in movie_details['release_dates']['results']:\n",
    "            if country_release['iso_3166_1'] == 'US':\n",
    "                for release in country_release['release_dates']:\n",
    "                    if release.get('type') in [2, 3]:  # Theatrical release\n",
    "                        us_release_date = release.get('release_date')\n",
    "                        us_certification = release.get('certification')\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    # Extract cast (top 5 actors)\n",
    "    cast = []\n",
    "    if 'credits' in movie_details and 'cast' in movie_details['credits']:\n",
    "        cast = [\n",
    "            {\n",
    "                'id': actor['id'],\n",
    "                'name': actor['name'],\n",
    "                'order': actor['order']\n",
    "            }\n",
    "            for actor in movie_details['credits']['cast'][:5]\n",
    "        ]\n",
    "    \n",
    "    # Extract director and crew\n",
    "    director = None\n",
    "    if 'credits' in movie_details and 'crew' in movie_details['credits']:\n",
    "        for crew_member in movie_details['credits']['crew']:\n",
    "            if crew_member['job'] == 'Director':\n",
    "                director = {\n",
    "                    'id': crew_member['id'],\n",
    "                    'name': crew_member['name']\n",
    "                }\n",
    "                break\n",
    "    \n",
    "    # Extract YouTube trailer key\n",
    "    trailer_key = None\n",
    "    if 'videos' in movie_details and 'results' in movie_details['videos']:\n",
    "        for video in movie_details['videos']['results']:\n",
    "            if video['type'] == 'Trailer' and video['site'] == 'YouTube':\n",
    "                trailer_key = video['key']\n",
    "                break\n",
    "    \n",
    "    # Extract genres\n",
    "    genres = [genre['name'] for genre in movie_details.get('genres', [])]\n",
    "    \n",
    "    # Extract production companies\n",
    "    production_companies = [company['name'] for company in movie_details.get('production_companies', [])]\n",
    "    \n",
    "    return {\n",
    "        'tmdb_id': movie_details.get('id'),\n",
    "        'imdb_id': movie_details.get('imdb_id'),\n",
    "        'title': movie_details.get('title'),\n",
    "        'original_title': movie_details.get('original_title'),\n",
    "        'release_date': movie_details.get('release_date'),\n",
    "        'us_release_date': us_release_date,\n",
    "        'us_certification': us_certification,\n",
    "        'budget': movie_details.get('budget'),\n",
    "        'revenue': movie_details.get('revenue'),  # Note: TMDB revenue often incomplete\n",
    "        'runtime': movie_details.get('runtime'),\n",
    "        'genres': '|'.join(genres) if genres else None,\n",
    "        'primary_genre': genres[0] if genres else None,\n",
    "        'num_genres': len(genres),\n",
    "        'popularity': movie_details.get('popularity'),\n",
    "        'vote_average': movie_details.get('vote_average'),\n",
    "        'vote_count': movie_details.get('vote_count'),\n",
    "        'director_id': director['id'] if director else None,\n",
    "        'director_name': director['name'] if director else None,\n",
    "        'cast_ids': '|'.join([str(actor['id']) for actor in cast]),\n",
    "        'cast_names': '|'.join([actor['name'] for actor in cast]),\n",
    "        'production_companies': '|'.join(production_companies) if production_companies else None,\n",
    "        'num_production_companies': len(production_companies),\n",
    "        'original_language': movie_details.get('original_language'),\n",
    "        'production_countries': '|'.join([country['iso_3166_1'] for country in movie_details.get('production_countries', [])]),\n",
    "        'youtube_trailer_key': trailer_key,\n",
    "        'tagline': movie_details.get('tagline'),\n",
    "        'overview': movie_details.get('overview')\n",
    "    }\n",
    "\n",
    "def collect_movies_for_year_range(start_year, end_year, pages_per_year=5):\n",
    "    \"\"\"\n",
    "    Collect movie data for a range of years.\n",
    "    \n",
    "    Args:\n",
    "        start_year: Starting year (inclusive)\n",
    "        end_year: Ending year (inclusive)\n",
    "        pages_per_year: Number of pages to fetch per year\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collected movie data\n",
    "    \"\"\"\n",
    "    all_movies = []\n",
    "    total_movies = 0\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\n=== Collecting movies for {year} ===\")\n",
    "        \n",
    "        # Get movie IDs for this year\n",
    "        movie_ids = get_popular_movies_by_year(year, pages=pages_per_year)\n",
    "        print(f\"  Found {len(movie_ids)} movie IDs for {year}\")\n",
    "        \n",
    "        # Get details for each movie\n",
    "        year_movies = 0\n",
    "        for i, movie_id in enumerate(movie_ids, 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Progress: {i}/{len(movie_ids)} movies processed for {year}\")\n",
    "            \n",
    "            movie_details = get_movie_details(movie_id)\n",
    "            if movie_details:\n",
    "                extracted_data = extract_movie_data(movie_details)\n",
    "                if extracted_data:\n",
    "                    all_movies.append(extracted_data)\n",
    "                    year_movies += 1\n",
    "        \n",
    "        print(f\"  Collected {year_movies} movies for {year}\")\n",
    "        total_movies += year_movies\n",
    "        print(f\"  Total movies collected so far: {total_movies}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_movies)\n",
    "    return df\n",
    "\n",
    "print(\"Data collection functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# OMDb API functions for budget collection\n\nclass OMDbRateLimiter:\n    \"\"\"Rate limiter for OMDb API (1,000 requests per day limit).\"\"\"\n    def __init__(self, max_calls_per_day=1000):\n        self.max_calls_per_day = max_calls_per_day\n        self.calls_today = 0\n        self.last_reset = datetime.now().date()\n        self.min_delay = 1.0  # Minimum 1 second between requests\n        self.last_call = 0\n    \n    def wait_if_needed(self):\n        \"\"\"Wait if rate limit reached or if minimum delay not elapsed.\"\"\"\n        # Check if we need to reset daily counter\n        today = datetime.now().date()\n        if today > self.last_reset:\n            self.calls_today = 0\n            self.last_reset = today\n            print(f\"  Daily rate limit reset. New day: {today}\")\n        \n        # Check daily limit\n        if self.calls_today >= self.max_calls_per_day:\n            # Calculate time until midnight\n            now = datetime.now()\n            tomorrow = datetime.combine(today + pd.Timedelta(days=1), datetime.min.time())\n            wait_seconds = (tomorrow - now).total_seconds()\n            print(f\"  Daily rate limit reached ({self.max_calls_per_day} requests).\")\n            print(f\"  Waiting until midnight ({wait_seconds/3600:.1f} hours)...\")\n            time.sleep(wait_seconds + 1)\n            self.calls_today = 0\n            self.last_reset = datetime.now().date()\n        \n        # Enforce minimum delay between requests\n        now = time.time()\n        elapsed = now - self.last_call\n        if elapsed < self.min_delay:\n            time.sleep(self.min_delay - elapsed)\n        \n        self.last_call = time.time()\n        self.calls_today += 1\n\n\ndef parse_omdb_budget(budget_str):\n    \"\"\"\n    Parse OMDb budget string to integer.\n    \n    Handles formats:\n    - \"$50,000,000\" → 50000000\n    - \"$50 million\" → 50000000\n    - \"50000000\" → 50000000\n    - \"N/A\" → None\n    - Missing/empty → None\n    \n    Args:\n        budget_str: Budget string from OMDb API\n    \n    Returns:\n        Integer budget value or None\n    \"\"\"\n    if not budget_str or budget_str == 'N/A':\n        return None\n    \n    try:\n        # Remove $ and commas\n        cleaned = budget_str.replace('$', '').replace(',', '').strip()\n        \n        # Handle \"X million\" format\n        if 'million' in cleaned.lower():\n            number = float(cleaned.lower().replace('million', '').strip())\n            return int(number * 1_000_000)\n        \n        # Handle \"X billion\" format (rare)\n        if 'billion' in cleaned.lower():\n            number = float(cleaned.lower().replace('billion', '').strip())\n            return int(number * 1_000_000_000)\n        \n        # Handle plain number\n        return int(float(cleaned))\n    \n    except (ValueError, AttributeError):\n        return None\n\n\ndef get_omdb_data(imdb_id, api_key):\n    \"\"\"\n    Get budget data for a movie from OMDb API.\n    \n    Args:\n        imdb_id: IMDb ID (e.g., 'tt1375666')\n        api_key: OMDb API key\n    \n    Returns:\n        Dictionary with imdb_id, omdb_budget, budget_raw, error\n    \"\"\"\n    url = \"http://www.omdbapi.com/\"\n    params = {\n        'i': imdb_id,\n        'apikey': api_key,\n        'plot': 'short'  # Minimize response size\n    }\n    \n    result = {\n        'imdb_id': imdb_id,\n        'omdb_budget': None,\n        'budget_raw': None,\n        'omdb_success': False,\n        'omdb_error': None\n    }\n    \n    try:\n        response = requests.get(url, params=params, timeout=10)\n        \n        if response.status_code == 200:\n            data = response.json()\n            \n            # Check if movie was found\n            if data.get('Response') == 'False':\n                result['omdb_error'] = data.get('Error', 'not_found')\n                return result\n            \n            # Extract budget\n            budget_str = data.get('BoxOffice') or data.get('Budget')  # Try BoxOffice first, then Budget field\n            \n            # Actually, OMDb doesn't have a \"Budget\" field - it's typically not in their data\n            # Let me check the actual response structure - OMDb mainly has: BoxOffice (revenue), not budget\n            # I need to reconsider this - OMDb API doesn't actually provide budget data!\n            \n            # NOTE: After checking OMDb API documentation, budget is NOT a standard field\n            # OMDb primarily provides: Title, Year, Rated, Released, Runtime, Genre, Director, \n            # Writer, Actors, Plot, Language, Country, Awards, Poster, Ratings, Metascore, \n            # imdbRating, imdbVotes, imdbID, Type, DVD, BoxOffice (revenue, not budget), \n            # Production, Website, Response\n            \n            # So this function won't work as intended. We need to flag this.\n            result['omdb_error'] = 'budget_field_not_available'\n            return result\n            \n        elif response.status_code == 401:\n            result['omdb_error'] = 'invalid_api_key'\n            return result\n        elif response.status_code == 429:\n            result['omdb_error'] = 'rate_limited'\n            return result\n        else:\n            result['omdb_error'] = f'http_{response.status_code}'\n            return result\n    \n    except requests.Timeout:\n        result['omdb_error'] = 'timeout'\n        return result\n    except Exception as e:\n        result['omdb_error'] = f'exception_{str(e)[:30]}'\n        return result\n\n\nprint(\"⚠️  WARNING: OMDb API does NOT provide budget data!\")\nprint(\"The OMDb API documentation shows they provide BoxOffice (revenue) but NOT budget.\")\nprint(\"Budget field is not available in OMDb responses.\")\nprint(\"\\nRecommendation: Skip OMDb for budget collection and proceed directly to Wikipedia scraping.\")\nprint(\"See plan for alternative budget sources: Wikipedia, The Numbers, etc.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## OMDb Budget Collection Functions\n\n### Purpose\nCollect additional budget data from OMDb API to fill gaps in TMDB/BOM budget coverage. Currently only 44% of movies have budget data, limiting our complete dataset to ~2,258 movies. OMDb can provide an estimated +300-500 additional budgets.\n\n### Strategy\n- Target movies with revenue_final > 0 but no budget_final (3,842 movies)\n- Use OMDb API rate limit: 1,000 requests/day\n- Parse budget strings: \"$50,000,000\" → 50000000\n- Handle various formats: \"$50 million\", \"N/A\", etc.\n- Runtime: ~4 days (1,000/day limit) with checkpointing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Collect TMDB data for movies from 2010-2024\n",
    "# This will take a while due to rate limiting (approximately 2-3 hours)\n",
    "\n",
    "# Set parameters\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2024\n",
    "PAGES_PER_YEAR = 17  # 17 pages x 20 movies = ~340 movies per year x 15 years = ~5,100 movies\n",
    "\n",
    "print(f\"Starting data collection for {START_YEAR}-{END_YEAR}\")\n",
    "print(f\"Fetching {PAGES_PER_YEAR} pages per year (~{PAGES_PER_YEAR * 20} movies/year)\")\n",
    "print(f\"Estimated total movies: ~{(END_YEAR - START_YEAR + 1) * PAGES_PER_YEAR * 20}\")\n",
    "print(f\"This will take approximately 2-3 hours due to API rate limiting.\\n\")\n",
    "\n",
    "# Collect the data\n",
    "start_time = time.time()\n",
    "df_tmdb = collect_movies_for_year_range(START_YEAR, END_YEAR, pages_per_year=PAGES_PER_YEAR)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Data collection complete!\")\n",
    "print(f\"Total movies collected: {len(df_tmdb)}\")\n",
    "print(f\"Time elapsed: {(end_time - start_time) / 60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Create data/raw directory if it doesn't exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'data/raw/movies_tmdb_raw.csv'\n",
    "df_tmdb.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_tmdb)}\")\n",
    "print(f\"Total columns: {len(df_tmdb.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Basic data inspection\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nShape: {df_tmdb.shape}\")\n",
    "print(f\"  Rows (movies): {df_tmdb.shape[0]}\")\n",
    "print(f\"  Columns (features): {df_tmdb.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN NAMES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = df_tmdb.isnull().sum()\n",
    "missing_pct = (missing / len(df_tmdb) * 100).round(1)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASIC STATISTICS (Numeric Columns)\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Movies with budget data: {df_tmdb['budget'].notna().sum()} ({df_tmdb['budget'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero budget: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with revenue data: {df_tmdb['revenue'].notna().sum()} ({df_tmdb['revenue'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero revenue: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with IMDb ID: {df_tmdb['imdb_id'].notna().sum()} ({df_tmdb['imdb_id'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with director: {df_tmdb['director_name'].notna().sum()} ({df_tmdb['director_name'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with cast data: {df_tmdb['cast_names'].notna().sum()} ({df_tmdb['cast_names'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with YouTube trailer: {df_tmdb['youtube_trailer_key'].notna().sum()} ({df_tmdb['youtube_trailer_key'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE MOVIES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb[['title', 'release_date', 'budget', 'revenue', 'primary_genre', 'director_name']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmdb_id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>original_title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>us_release_date</th>\n",
       "      <th>us_certification</th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>...</th>\n",
       "      <th>director_name</th>\n",
       "      <th>cast_ids</th>\n",
       "      <th>cast_names</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>num_production_companies</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>youtube_trailer_key</th>\n",
       "      <th>tagline</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27205</td>\n",
       "      <td>tt1375666</td>\n",
       "      <td>Inception</td>\n",
       "      <td>Inception</td>\n",
       "      <td>2010-07-15</td>\n",
       "      <td>2010-07-16T00:00:00.000Z</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>160000000</td>\n",
       "      <td>839030630</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>6193|24045|3899|2524|27578</td>\n",
       "      <td>Leonardo DiCaprio|Joseph Gordon-Levitt|Ken Wat...</td>\n",
       "      <td>Legendary Pictures|Syncopy|Warner Bros. Pictures</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>GB|US</td>\n",
       "      <td>cdx31ak4KbQ</td>\n",
       "      <td>Your mind is the scene of the crime.</td>\n",
       "      <td>Cobb, a skilled thief who commits corporate es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10138</td>\n",
       "      <td>tt1228705</td>\n",
       "      <td>Iron Man 2</td>\n",
       "      <td>Iron Man 2</td>\n",
       "      <td>2010-04-28</td>\n",
       "      <td>2010-05-07T00:00:00.000Z</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>200000000</td>\n",
       "      <td>623933331</td>\n",
       "      <td>124</td>\n",
       "      <td>...</td>\n",
       "      <td>Jon Favreau</td>\n",
       "      <td>3223|12052|1896|1245|6807</td>\n",
       "      <td>Robert Downey Jr.|Gwyneth Paltrow|Don Cheadle|...</td>\n",
       "      <td>Marvel Studios|Fairview Entertainment|Marvel E...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>BoohRoVA9WQ</td>\n",
       "      <td>It's not the armor that makes the hero, but th...</td>\n",
       "      <td>With the world now aware of his dual life as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38757</td>\n",
       "      <td>tt0398286</td>\n",
       "      <td>Tangled</td>\n",
       "      <td>Tangled</td>\n",
       "      <td>2010-11-24</td>\n",
       "      <td>2010-11-24T00:00:00.000Z</td>\n",
       "      <td>PG</td>\n",
       "      <td>260000000</td>\n",
       "      <td>592461732</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>Byron Howard</td>\n",
       "      <td>16855|69899|2517|2372|22132</td>\n",
       "      <td>Mandy Moore|Zachary Levi|Donna Murphy|Ron Perl...</td>\n",
       "      <td>Walt Disney Animation Studios|Walt Disney Pict...</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>gsYKF8ecC8g</td>\n",
       "      <td>They're taking adventure to new lengths.</td>\n",
       "      <td>Feisty teenager Rapunzel, who has long and mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12444</td>\n",
       "      <td>tt0926084</td>\n",
       "      <td>Harry Potter and the Deathly Hallows: Part 1</td>\n",
       "      <td>Harry Potter and the Deathly Hallows: Part 1</td>\n",
       "      <td>2010-11-17</td>\n",
       "      <td>2010-11-19T00:00:00.000Z</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>250000000</td>\n",
       "      <td>954305868</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>David Yates</td>\n",
       "      <td>10980|10990|10989|13014|1283</td>\n",
       "      <td>Daniel Radcliffe|Emma Watson|Rupert Grint|Toby...</td>\n",
       "      <td>Warner Bros. Pictures|Heyday Films</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>GB|US</td>\n",
       "      <td>Su1LOpjvdZ4</td>\n",
       "      <td>Nowhere is safe.</td>\n",
       "      <td>Harry, Ron and Hermione walk away from their l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10191</td>\n",
       "      <td>tt0892769</td>\n",
       "      <td>How to Train Your Dragon</td>\n",
       "      <td>How to Train Your Dragon</td>\n",
       "      <td>2010-03-18</td>\n",
       "      <td>2010-03-26T00:00:00.000Z</td>\n",
       "      <td>PG</td>\n",
       "      <td>165000000</td>\n",
       "      <td>495141736</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>Chris Sanders</td>\n",
       "      <td>449|17276|24264|59174|21007</td>\n",
       "      <td>Jay Baruchel|Gerard Butler|Craig Ferguson|Amer...</td>\n",
       "      <td>DreamWorks Animation</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>KZtbJ_I9IFM</td>\n",
       "      <td>One adventure will change two worlds.</td>\n",
       "      <td>As the son of a Viking leader on the cusp of m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tmdb_id    imdb_id                                         title  \\\n",
       "0    27205  tt1375666                                     Inception   \n",
       "1    10138  tt1228705                                    Iron Man 2   \n",
       "2    38757  tt0398286                                       Tangled   \n",
       "3    12444  tt0926084  Harry Potter and the Deathly Hallows: Part 1   \n",
       "4    10191  tt0892769                      How to Train Your Dragon   \n",
       "\n",
       "                                 original_title release_date  \\\n",
       "0                                     Inception   2010-07-15   \n",
       "1                                    Iron Man 2   2010-04-28   \n",
       "2                                       Tangled   2010-11-24   \n",
       "3  Harry Potter and the Deathly Hallows: Part 1   2010-11-17   \n",
       "4                      How to Train Your Dragon   2010-03-18   \n",
       "\n",
       "            us_release_date us_certification     budget    revenue  runtime  \\\n",
       "0  2010-07-16T00:00:00.000Z            PG-13  160000000  839030630      148   \n",
       "1  2010-05-07T00:00:00.000Z            PG-13  200000000  623933331      124   \n",
       "2  2010-11-24T00:00:00.000Z               PG  260000000  592461732      100   \n",
       "3  2010-11-19T00:00:00.000Z            PG-13  250000000  954305868      146   \n",
       "4  2010-03-26T00:00:00.000Z               PG  165000000  495141736       98   \n",
       "\n",
       "   ...      director_name                      cast_ids  \\\n",
       "0  ...  Christopher Nolan    6193|24045|3899|2524|27578   \n",
       "1  ...        Jon Favreau     3223|12052|1896|1245|6807   \n",
       "2  ...       Byron Howard   16855|69899|2517|2372|22132   \n",
       "3  ...        David Yates  10980|10990|10989|13014|1283   \n",
       "4  ...      Chris Sanders   449|17276|24264|59174|21007   \n",
       "\n",
       "                                          cast_names  \\\n",
       "0  Leonardo DiCaprio|Joseph Gordon-Levitt|Ken Wat...   \n",
       "1  Robert Downey Jr.|Gwyneth Paltrow|Don Cheadle|...   \n",
       "2  Mandy Moore|Zachary Levi|Donna Murphy|Ron Perl...   \n",
       "3  Daniel Radcliffe|Emma Watson|Rupert Grint|Toby...   \n",
       "4  Jay Baruchel|Gerard Butler|Craig Ferguson|Amer...   \n",
       "\n",
       "                                production_companies  \\\n",
       "0   Legendary Pictures|Syncopy|Warner Bros. Pictures   \n",
       "1  Marvel Studios|Fairview Entertainment|Marvel E...   \n",
       "2  Walt Disney Animation Studios|Walt Disney Pict...   \n",
       "3                 Warner Bros. Pictures|Heyday Films   \n",
       "4                               DreamWorks Animation   \n",
       "\n",
       "   num_production_companies  original_language  production_countries  \\\n",
       "0                         3                 en                 GB|US   \n",
       "1                         3                 en                    US   \n",
       "2                         2                 en                    US   \n",
       "3                         2                 en                 GB|US   \n",
       "4                         1                 en                    US   \n",
       "\n",
       "  youtube_trailer_key                                            tagline  \\\n",
       "0         cdx31ak4KbQ               Your mind is the scene of the crime.   \n",
       "1         BoohRoVA9WQ  It's not the armor that makes the hero, but th...   \n",
       "2         gsYKF8ecC8g           They're taking adventure to new lengths.   \n",
       "3         Su1LOpjvdZ4                                   Nowhere is safe.   \n",
       "4         KZtbJ_I9IFM              One adventure will change two worlds.   \n",
       "\n",
       "                                            overview  \n",
       "0  Cobb, a skilled thief who commits corporate es...  \n",
       "1  With the world now aware of his dual life as t...  \n",
       "2  Feisty teenager Rapunzel, who has long and mag...  \n",
       "3  Harry, Ron and Hermione walk away from their l...  \n",
       "4  As the son of a Viking leader on the cusp of m...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11\n",
    "df_tmdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Box Office Mojo Revenue Scraping\n",
    "\n",
    "### Purpose\n",
    "Scrape box office revenue data from Box Office Mojo to fill gaps in TMDB data. Currently only 37% of movies have both budget and revenue, limiting our dataset size. Box Office Mojo provides comprehensive revenue data including opening weekend, domestic total, international, and worldwide gross.\n",
    "\n",
    "### Strategy\n",
    "- Target all 5,094 movies with IMDb IDs\n",
    "- Use respectful rate limiting (1.5s delays)\n",
    "- Implement checkpointing for resumability\n",
    "- Extract: opening weekend, domestic total, international total, worldwide total, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing TMDB data from CSV...\n",
      "Loaded 5100 movies\n"
     ]
    }
   ],
   "source": [
    "# Cell 13\n",
    "# Load existing TMDB data if not already in memory\n",
    "if 'df_tmdb' not in locals():\n",
    "    print(\"Loading existing TMDB data from CSV...\")\n",
    "    df_tmdb = pd.read_csv('data/raw/movies_tmdb_raw.csv')\n",
    "    print(f\"Loaded {len(df_tmdb)} movies\")\n",
    "else:\n",
    "    print(f\"df_tmdb already in memory with {len(df_tmdb)} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML parsing function loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14\n",
    "def parse_bom_revenue(soup, imdb_id):\n",
    "    \"\"\"\n",
    "    Extract revenue from Box Office Mojo HTML using span.money tags.\n",
    "\n",
    "    BOM structure: Revenue values are in <span class=\"money\"> tags.\n",
    "    We find all money spans and match them to labels by proximity.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object of page HTML\n",
    "        imdb_id: IMDb ID for result dictionary\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with revenue fields or None values if not found\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': True,\n",
    "        'error_message': None\n",
    "    }\n",
    "\n",
    "    # Find all <span class=\"money\"> elements\n",
    "    money_spans = soup.find_all('span', class_='money')\n",
    "\n",
    "    # For each money span, check surrounding context for labels\n",
    "    for money_span in money_spans:\n",
    "        # Get the dollar amount\n",
    "        amount_text = money_span.get_text(strip=True)\n",
    "        if not amount_text or amount_text == '–':\n",
    "            continue\n",
    "\n",
    "        amount = int(amount_text.replace('$', '').replace(',', ''))\n",
    "\n",
    "        # Get parent div to find associated label\n",
    "        parent = money_span.find_parent('div', class_='a-section')\n",
    "        if not parent:\n",
    "            continue\n",
    "\n",
    "        # Get the text of the parent div to find label\n",
    "        parent_text = parent.get_text().lower()\n",
    "\n",
    "        # Match to appropriate field based on label in parent\n",
    "        # Use \"not result[field]\" to only capture the first occurrence\n",
    "        if 'worldwide' in parent_text and result['worldwide_total'] is None:\n",
    "            result['worldwide_total'] = amount\n",
    "        elif 'domestic' in parent_text and 'international' not in parent_text and result['domestic_total'] is None:\n",
    "            result['domestic_total'] = amount\n",
    "        elif 'international' in parent_text and result['international_total'] is None:\n",
    "            result['international_total'] = amount\n",
    "        elif 'opening' in parent_text and result['opening_weekend'] is None:\n",
    "            result['opening_weekend'] = amount\n",
    "        elif 'budget' in parent_text and result['bom_budget'] is None:\n",
    "            result['bom_budget'] = amount\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"HTML parsing function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15\n",
    "def clean_currency(currency_str):\n",
    "    \"\"\"Helper function to convert currency string to integer.\"\"\"\n",
    "    return int(currency_str.replace('$', '').replace(',', ''))\n",
    "\n",
    "def error_result(imdb_id, error_type):\n",
    "    \"\"\"Helper function to create error result dictionary.\"\"\"\n",
    "    return {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': False,\n",
    "        'error_message': error_type\n",
    "    }\n",
    "\n",
    "class BOMRateLimiter:\n",
    "    \"\"\"Rate limiter for Box Office Mojo scraping.\"\"\"\n",
    "    def __init__(self, delay=1.5):\n",
    "        self.delay = delay\n",
    "        self.last_call = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        elapsed = time.time() - self.last_call\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_call = time.time()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main scraping function loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 16\n",
    "def scrape_bom_movie(imdb_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrape revenue data for a single movie from Box Office Mojo.\n",
    "    \n",
    "    Handles various error conditions:\n",
    "    - 404: Movie not found in BOM\n",
    "    - 429: Rate limited (exponential backoff)\n",
    "    - 5xx: Server errors (retry with delays)\n",
    "    - Timeout: Network timeout (retry once)\n",
    "    - Other exceptions: Catch and log\n",
    "    \n",
    "    Args:\n",
    "        imdb_id: IMDb ID (e.g., 'tt1375666')\n",
    "        max_retries: Maximum retry attempts for recoverable errors\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with revenue data or error information\n",
    "    \"\"\"\n",
    "    url = f\"https://www.boxofficemojo.com/title/{imdb_id}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; MovieDataCollector/1.0)'}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "            if response.status_code == 404:\n",
    "                # Movie not in Box Office Mojo database\n",
    "                return error_result(imdb_id, 'not_found')\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited - wait with exponential backoff\n",
    "                wait = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                print(f\"  Rate limited for {imdb_id}, waiting {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            elif response.status_code >= 500:\n",
    "                # Server error - retry if attempts remain\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  Server error {response.status_code} for {imdb_id}, retrying...\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                return error_result(imdb_id, f'server_error_{response.status_code}')\n",
    "\n",
    "            elif response.status_code == 200:\n",
    "                # Success - parse the HTML\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                return parse_bom_revenue(soup, imdb_id)\n",
    "\n",
    "            else:\n",
    "                # Unexpected status code\n",
    "                return error_result(imdb_id, f'http_{response.status_code}')\n",
    "\n",
    "        except requests.Timeout:\n",
    "            # Network timeout - retry if attempts remain\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  Timeout for {imdb_id}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return error_result(imdb_id, 'timeout')\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch-all for unexpected errors\n",
    "            error_msg = str(e)[:50]  # Truncate long error messages\n",
    "            return error_result(imdb_id, f'exception_{error_msg}')\n",
    "\n",
    "    # Max retries exhausted\n",
    "    return error_result(imdb_id, 'max_retries')\n",
    "\n",
    "print(\"Main scraping function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing function loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 17\n",
    "def scrape_bom_batch(imdb_ids, checkpoint_file='data/raw/bom_checkpoint.csv', save_every=100):\n",
    "    \"\"\"\n",
    "    Scrape multiple movies with automatic checkpointing for resumability.\n",
    "    \n",
    "    If interrupted, the function can resume from the last checkpoint by\n",
    "    simply running again - it will load completed IMDb IDs and skip them.\n",
    "    \n",
    "    Args:\n",
    "        imdb_ids: List of IMDb IDs to scrape\n",
    "        checkpoint_file: Path to save progress (CSV format)\n",
    "        save_every: Save checkpoint every N movies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all scraped results\n",
    "    \"\"\"\n",
    "    # Load existing checkpoint if available\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "        completed = set(df_checkpoint['imdb_id'].dropna())\n",
    "        results = df_checkpoint.to_dict('records')\n",
    "        print(f\"Resuming from checkpoint: {len(completed)} already scraped\")\n",
    "    else:\n",
    "        completed = set()\n",
    "        results = []\n",
    "        print(\"Starting fresh scrape (no checkpoint found)\")\n",
    "\n",
    "    # Filter to unscraped movies\n",
    "    remaining = [id for id in imdb_ids if id not in completed]\n",
    "    print(f\"Scraping {len(remaining)} movies...\")\n",
    "    print(f\"Estimated time: {len(remaining) * 2 / 3600:.1f} hours\\n\")\n",
    "\n",
    "    # Initialize rate limiter\n",
    "    limiter = BOMRateLimiter(delay=1.5)\n",
    "\n",
    "    # Scrape each movie\n",
    "    for i, imdb_id in enumerate(remaining, 1):\n",
    "        limiter.wait()  # Respect rate limit before each request\n",
    "\n",
    "        result = scrape_bom_movie(imdb_id)\n",
    "        results.append(result)\n",
    "\n",
    "        # Progress report every 50 movies\n",
    "        if i % 50 == 0:\n",
    "            success = sum(1 for r in results[-i:] if r['scrape_success'])\n",
    "            print(f\"  Progress: {i}/{len(remaining)} | Recent success rate: {success}/{min(i, 50)} ({success/min(i, 50)*100:.1f}%)\")\n",
    "\n",
    "        # Save checkpoint every N movies\n",
    "        if i % save_every == 0:\n",
    "            pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "            print(f\"  Checkpoint saved ({len(results)} total movies)\")\n",
    "\n",
    "    # Final save\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(checkpoint_file, index=False)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    total_success = df['scrape_success'].sum()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping complete!\")\n",
    "    print(f\"  Total movies: {len(df)}\")\n",
    "    print(f\"  Successful: {total_success} ({total_success/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Failed: {len(df) - total_success}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Batch processing function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Scraping on Sample Movies\n",
    "\n",
    "Before running the full 3-4 hour scraping job, test on a few movies to verify the scraping logic works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 - TEST SCRAPING ON 5 MOVIES\n",
    "# Test scraper on 5 sample movies\n",
    "print(\"Testing Box Office Mojo scraper on sample movies...\\n\")\n",
    "\n",
    "test_ids = df_tmdb.head(5)['imdb_id'].dropna().tolist()\n",
    "\n",
    "for test_id in test_ids:\n",
    "    result = scrape_bom_movie(test_id)\n",
    "    \n",
    "    # Get movie title for context\n",
    "    title = df_tmdb[df_tmdb['imdb_id'] == test_id]['title'].values[0]\n",
    "    \n",
    "    print(f\"{test_id} ({title}):\")\n",
    "    print(f\"  Success: {result['scrape_success']}\")\n",
    "    print(f\"  Worldwide: ${result['worldwide_total']:,}\" if result['worldwide_total'] else f\"  Worldwide: None\")\n",
    "    print(f\"  Domestic: ${result['domestic_total']:,}\" if result['domestic_total'] else f\"  Domestic: None\")\n",
    "    print(f\"  Opening: ${result['opening_weekend']:,}\" if result['opening_weekend'] else f\"  Opening: None\")\n",
    "    if result['error_message']:\n",
    "        print(f\"  Error: {result['error_message']}\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(1.5)  # Rate limit during test\n",
    "\n",
    "print(\"Test complete! If results look good, proceed to full scraping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Full Box Office Mojo Scraping\n",
    "\n",
    "**Note:** This will take approximately 3-4 hours. The scraper uses checkpointing, so it can be safely interrupted and resumed. Consider running overnight or during a long break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Box Office Mojo scraping...\n",
      "Total movies to scrape: 5094\n",
      "Estimated time: 2.8 hours\n",
      "Started at: 2026-01-22 15:28:02\n",
      "\n",
      "Resuming from checkpoint: 5094 already scraped\n",
      "Scraping 0 movies...\n",
      "Estimated time: 0.0 hours\n",
      "\n",
      "\n",
      "============================================================\n",
      "Scraping complete!\n",
      "  Total movies: 5094\n",
      "  Successful: 5094 (100.0%)\n",
      "  Failed: 0\n",
      "============================================================\n",
      "\n",
      "Finished at: 2026-01-22 15:28:02\n",
      "Total time: 0.0 minutes (0.00 hours)\n"
     ]
    }
   ],
   "source": [
    "# Cell 21\n",
    "# Get all IMDb IDs from TMDB data\n",
    "imdb_ids = df_tmdb['imdb_id'].dropna().tolist()\n",
    "\n",
    "print(f\"Starting Box Office Mojo scraping...\")\n",
    "print(f\"Total movies to scrape: {len(imdb_ids)}\")\n",
    "print(f\"Estimated time: {len(imdb_ids) * 2 / 3600:.1f} hours\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Run batch scraper with checkpointing\n",
    "scrape_start = time.time()\n",
    "df_bom = scrape_bom_batch(\n",
    "    imdb_ids,\n",
    "    checkpoint_file='data/raw/bom_checkpoint.csv',\n",
    "    save_every=100\n",
    ")\n",
    "scrape_end = time.time()\n",
    "\n",
    "print(f\"\\nFinished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {(scrape_end - scrape_start) / 60:.1f} minutes ({(scrape_end - scrape_start) / 3600:.2f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Raw Box Office Mojo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box Office Mojo data saved to: data/raw/revenue_boxofficemojo_raw.csv\n",
      "File size: 234.9 KB\n",
      "Total rows: 5094\n",
      "Total columns: 8\n",
      "\n",
      "First 5 rows:\n",
      "     imdb_id  domestic_total  opening_weekend  international_total  \\\n",
      "0  tt1375666     292587330.0       62785337.0          547199143.0   \n",
      "1  tt1228705     312433331.0      128122480.0          311500000.0   \n",
      "2  tt0398286     200821936.0       48767052.0          390974084.0   \n",
      "3  tt0926084     296374621.0      125017372.0          664444507.0   \n",
      "4  tt0892769     217581231.0       43732319.0          277298240.0   \n",
      "\n",
      "   worldwide_total   bom_budget  scrape_success  error_message  \n",
      "0      839786473.0  160000000.0            True            NaN  \n",
      "1      623933331.0  200000000.0            True            NaN  \n",
      "2      591806017.0  260000000.0            True            NaN  \n",
      "3      960858478.0          NaN            True            NaN  \n",
      "4      494879860.0  165000000.0            True            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Cell 23\n",
    "# Save Box Office Mojo data as raw CSV\n",
    "output_file = 'data/raw/revenue_boxofficemojo_raw.csv'\n",
    "df_bom.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Box Office Mojo data saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_bom)}\")\n",
    "print(f\"Total columns: {len(df_bom.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_bom.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Merge TMDB and Box Office Mojo Data\n",
    "\n",
    "Combine the two data sources, preferring BOM revenue (more complete) over TMDB revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MERGE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Merged dataset shape: (5100, 37)\n",
      "  Rows: 5100\n",
      "  Columns: 37\n",
      "\n",
      "Revenue source breakdown:\n",
      "revenue_source\n",
      "both    2630\n",
      "bom     1355\n",
      "none    1062\n",
      "tmdb      53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Budget source breakdown:\n",
      "budget_source\n",
      "none    2842\n",
      "tmdb    1484\n",
      "both     772\n",
      "bom        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New columns added:\n",
      "  - domestic_total (from BOM)\n",
      "  - opening_weekend (from BOM)\n",
      "  - worldwide_total (from BOM)\n",
      "  - international_total (from BOM)\n",
      "  - bom_budget (from BOM)\n",
      "  - revenue_final (combined best source)\n",
      "  - budget_final (combined best source)\n",
      "  - revenue_source (tracking field)\n",
      "  - budget_source (tracking field)\n"
     ]
    }
   ],
   "source": [
    "# Cell 25\n",
    "# Merge TMDB and BOM data on IMDb ID\n",
    "df_merged = df_tmdb.merge(\n",
    "    df_bom[['imdb_id', 'domestic_total', 'opening_weekend', 'worldwide_total', 'international_total', 'bom_budget', 'scrape_success']],\n",
    "    on='imdb_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create final revenue column - prefer BOM worldwide, fallback to TMDB revenue\n",
    "df_merged['revenue_final'] = df_merged['worldwide_total'].fillna(df_merged['revenue'])\n",
    "\n",
    "# Create final budget column - prefer BOM budget, fallback to TMDB budget\n",
    "df_merged['budget_final'] = df_merged['bom_budget'].fillna(df_merged['budget'])\n",
    "\n",
    "# Track revenue source for transparency\n",
    "df_merged['revenue_source'] = 'none'\n",
    "df_merged.loc[df_merged['revenue'] > 0, 'revenue_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['worldwide_total'].notna(), 'revenue_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna()), 'revenue_source'] = 'both'\n",
    "\n",
    "# Track budget source for transparency\n",
    "df_merged['budget_source'] = 'none'\n",
    "df_merged.loc[df_merged['budget'] > 0, 'budget_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['bom_budget'].notna(), 'budget_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['budget'] > 0) & (df_merged['bom_budget'].notna()), 'budget_source'] = 'both'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MERGE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"  Rows: {len(df_merged)}\")\n",
    "print(f\"  Columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\nRevenue source breakdown:\")\n",
    "print(df_merged['revenue_source'].value_counts())\n",
    "\n",
    "print(\"\\nBudget source breakdown:\")\n",
    "print(df_merged['budget_source'].value_counts())\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "print(\"  - domestic_total (from BOM)\")\n",
    "print(\"  - opening_weekend (from BOM)\")\n",
    "print(\"  - worldwide_total (from BOM)\")\n",
    "print(\"  - international_total (from BOM)\")\n",
    "print(\"  - bom_budget (from BOM)\")\n",
    "print(\"  - revenue_final (combined best source)\")\n",
    "print(\"  - budget_final (combined best source)\")\n",
    "print(\"  - revenue_source (tracking field)\")\n",
    "print(\"  - budget_source (tracking field)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quality Analysis\n",
    "\n",
    "Analyze scraping results, gap filling, and dataset completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BOX OFFICE MOJO SCRAPING RESULTS\n",
      "============================================================\n",
      "\n",
      "Scraping Success Rate:\n",
      "  Total attempted: 5094\n",
      "  Successful: 5094 (100.0%)\n",
      "  Failed: 0\n",
      "\n",
      "============================================================\n",
      "REVENUE COVERAGE\n",
      "============================================================\n",
      "\n",
      "Before BOM scraping:\n",
      "  TMDB revenue > 0: 2683 (52.6%)\n",
      "\n",
      "After BOM scraping:\n",
      "  BOM revenue available: 3985 (78.1%)\n",
      "  Final revenue > 0: 4038 (79.2%)\n",
      "\n",
      "Revenue Gap Filling:\n",
      "  TMDB revenue gaps: 2417\n",
      "  Gaps filled by BOM: 1355\n",
      "  Gap fill rate: 56.1%\n",
      "\n",
      "============================================================\n",
      "BUDGET COVERAGE\n",
      "============================================================\n",
      "\n",
      "Before BOM scraping:\n",
      "  TMDB budget > 0: 2256 (44.2%)\n",
      "\n",
      "After BOM scraping:\n",
      "  BOM budget available: 774 (15.2%)\n",
      "  Final budget > 0: 2258 (44.3%)\n",
      "\n",
      "Budget Gap Filling:\n",
      "  TMDB budget gaps: 2844\n",
      "  Gaps filled by BOM: 2\n",
      "  Gap fill rate: 0.1%\n",
      "\n",
      "============================================================\n",
      "REVENUE COMPARISON (Movies with Both Sources)\n",
      "============================================================\n",
      "\n",
      "Count: 2630\n",
      "Mean absolute difference: $3,168,057\n",
      "Median absolute difference: $5,544\n",
      "Mean % difference: 5167049.3%\n",
      "Median % difference: 0.0%\n",
      "\n",
      "Movies with >20% difference: 406 (15.4%)\n",
      "\n",
      "Example large discrepancies:\n",
      "  The Wandering Earth II: TMDB=$5 vs BOM=$615,023,132.0 (12300462540.0% diff)\n",
      "  The Zero Hour: TMDB=$2 vs BOM=$3,947,360.0 (197367900.0% diff)\n",
      "  Forever Young: TMDB=$1 vs BOM=$1,852,958.0 (185295700.0% diff)\n",
      "\n",
      "============================================================\n",
      "DATASET READINESS\n",
      "============================================================\n",
      "\n",
      "Before BOM scraping:\n",
      "  Movies with budget > 0: 2256\n",
      "  Movies with revenue > 0: 2683\n",
      "  Movies with BOTH budget & revenue: 1888 (37.0%)\n",
      "\n",
      "After BOM scraping:\n",
      "  Movies with budget_final > 0: 2258\n",
      "  Movies with revenue_final > 0: 4038\n",
      "  Movies with BOTH budget & revenue: 2097 (41.1%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 27\n",
    "print(\"=\"*60)\n",
    "print(\"BOX OFFICE MOJO SCRAPING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scraping success rate\n",
    "total_scraped = len(df_bom)\n",
    "successful = df_bom['scrape_success'].sum()\n",
    "print(f\"\\nScraping Success Rate:\")\n",
    "print(f\"  Total attempted: {total_scraped}\")\n",
    "print(f\"  Successful: {successful} ({successful/total_scraped*100:.1f}%)\")\n",
    "print(f\"  Failed: {total_scraped - successful}\")\n",
    "\n",
    "# Most common errors\n",
    "if (total_scraped - successful) > 0:\n",
    "    print(\"\\nMost common errors:\")\n",
    "    error_counts = df_bom[~df_bom['scrape_success']]['error_message'].value_counts().head(5)\n",
    "    for error, count in error_counts.items():\n",
    "        print(f\"  {error}: {count}\")\n",
    "\n",
    "# Revenue coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB revenue > 0: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM revenue available: {df_merged['worldwide_total'].notna().sum()} ({df_merged['worldwide_total'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final revenue > 0: {(df_merged['revenue_final'] > 0).sum()} ({(df_merged['revenue_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for revenue\n",
    "tmdb_missing = (df_merged['revenue'] == 0) | (df_merged['revenue'].isna())\n",
    "bom_filled = df_merged['worldwide_total'].notna()\n",
    "gaps_filled = (tmdb_missing & bom_filled).sum()\n",
    "\n",
    "print(f\"\\nRevenue Gap Filling:\")\n",
    "print(f\"  TMDB revenue gaps: {tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {gaps_filled/tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Budget coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BUDGET COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB budget > 0: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM budget available: {df_merged['bom_budget'].notna().sum()} ({df_merged['bom_budget'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final budget > 0: {(df_merged['budget_final'] > 0).sum()} ({(df_merged['budget_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for budget\n",
    "budget_tmdb_missing = (df_merged['budget'] == 0) | (df_merged['budget'].isna())\n",
    "budget_bom_filled = df_merged['bom_budget'].notna()\n",
    "budget_gaps_filled = (budget_tmdb_missing & budget_bom_filled).sum()\n",
    "\n",
    "print(f\"\\nBudget Gap Filling:\")\n",
    "print(f\"  TMDB budget gaps: {budget_tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {budget_gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {budget_gaps_filled/budget_tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Revenue comparison for movies with both sources\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COMPARISON (Movies with Both Sources)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "both = (df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna())\n",
    "if both.sum() > 0:\n",
    "    df_compare = df_merged[both].copy()\n",
    "    df_compare['diff'] = abs(df_compare['revenue'] - df_compare['worldwide_total'])\n",
    "    df_compare['diff_pct'] = df_compare['diff'] / df_compare['revenue'] * 100\n",
    "\n",
    "    print(f\"\\nCount: {len(df_compare)}\")\n",
    "    print(f\"Mean absolute difference: ${df_compare['diff'].mean():,.0f}\")\n",
    "    print(f\"Median absolute difference: ${df_compare['diff'].median():,.0f}\")\n",
    "    print(f\"Mean % difference: {df_compare['diff_pct'].mean():.1f}%\")\n",
    "    print(f\"Median % difference: {df_compare['diff_pct'].median():.1f}%\")\n",
    "    print(f\"\\nMovies with >20% difference: {(df_compare['diff_pct'] > 20).sum()} ({(df_compare['diff_pct'] > 20).sum()/len(df_compare)*100:.1f}%)\")\n",
    "    \n",
    "    # Show a few examples of large discrepancies\n",
    "    if (df_compare['diff_pct'] > 20).sum() > 0:\n",
    "        print(\"\\nExample large discrepancies:\")\n",
    "        large_diff = df_compare.nlargest(3, 'diff_pct')[['title', 'revenue', 'worldwide_total', 'diff_pct']]\n",
    "        for idx, row in large_diff.iterrows():\n",
    "            print(f\"  {row['title']}: TMDB=${row['revenue']:,} vs BOM=${row['worldwide_total']:,} ({row['diff_pct']:.1f}% diff)\")\n",
    "\n",
    "# Dataset readiness check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READINESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Before BOM\n",
    "complete_before = (df_tmdb['budget'] > 0) & (df_tmdb['revenue'] > 0)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  Movies with budget > 0: {(df_tmdb['budget'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue > 0: {(df_tmdb['revenue'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_before.sum()} ({complete_before.sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "# After BOM\n",
    "complete_after = (df_merged['budget_final'] > 0) & (df_merged['revenue_final'] > 0)\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  Movies with budget_final > 0: {(df_merged['budget_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue_final > 0: {(df_merged['revenue_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_after.sum()} ({complete_after.sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Save cleaned dataset\nprint(\"=\"*60)\nprint(\"SAVING CLEANED DATASET\")\nprint(\"=\"*60)\n\n# Overwrite the merged dataset file with cleaned version\ncleaned_output = 'data/raw/movies_merged.csv'\ndf_merged.to_csv(cleaned_output, index=False)\n\nprint(f\"\\n✅ Cleaned dataset saved to: {cleaned_output}\")\nprint(f\"   File size: {os.path.getsize(cleaned_output) / 1024:.1f} KB\")\nprint(f\"   Rows: {len(df_merged):,}\")\nprint(f\"   Columns: {len(df_merged.columns)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"NEXT STEPS\")\nprint(\"=\"*60)\nprint(\"\\n1. ✅ Column cleanup complete (dropped redundant, renamed, added empty columns)\")\nprint(\"\\n2. ⬜ Collect additional data to fill empty columns:\")\nprint(\"   - Theater counts: Enhance BOM scraper to extract opening_theaters, max_theaters\")\nprint(\"   - Franchise data: Use TMDB /movie/{id}/belongs_to_collection endpoint\")\nprint(\"   - Trailer metrics: Use YouTube Data API to get views and publish dates\")\nprint(\"\\n3. ⬜ Alternative budget sources (if needed to reach 3,500-4,000 complete movies):\")\nprint(\"   - Wikipedia infobox scraping (~500-800 additional budgets)\")\nprint(\"   - The Numbers website (~300-500 additional budgets)\")\nprint(\"\\n4. ⬜ Proceed to 02_data_cleaning_eda.ipynb for data cleaning and EDA\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display updated column list\nprint(\"=\"*60)\nprint(\"UPDATED COLUMN LIST\")\nprint(\"=\"*60)\nprint(f\"\\nTotal columns: {len(df_merged.columns)}\\n\")\n\nfor i, col in enumerate(df_merged.columns, 1):\n    print(f\"  {i:2d}. {col}\")\n\n# Show sample of data with key columns\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE DATA (Key Financial Columns)\")\nprint(\"=\"*60)\nkey_cols = ['title', 'release_date', 'budget', 'revenue_worldwide', 'domestic_total', 'opening_weekend']\nprint(df_merged[key_cols].head(10))\n\n# Check data completeness for new columns\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA COMPLETENESS CHECK\")\nprint(\"=\"*60)\n\ncompleteness = {\n    'budget': (df_merged['budget'] > 0).sum(),\n    'revenue_worldwide': (df_merged['revenue_worldwide'] > 0).sum(),\n    'domestic_total': df_merged['domestic_total'].notna().sum(),\n    'opening_weekend': df_merged['opening_weekend'].notna().sum(),\n    'both_budget_revenue': ((df_merged['budget'] > 0) & (df_merged['revenue_worldwide'] > 0)).sum()\n}\n\nprint(\"\\nMovies with complete data:\")\nfor field, count in completeness.items():\n    pct = count / len(df_merged) * 100\n    print(f\"  {field:25s}: {count:4d} ({pct:5.1f}%)\")\n\nprint(f\"\\n📊 Ready for modeling: {completeness['both_budget_revenue']} movies have both budget & revenue\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Drop redundant columns\nprint(\"=\"*60)\nprint(\"COLUMN CLEANUP\")\nprint(\"=\"*60)\n\nprint(f\"\\nStarting columns: {len(df_merged.columns)}\")\nprint(f\"Starting shape: {df_merged.shape}\")\n\n# Columns to drop (superseded by _final versions or not needed)\ncolumns_to_drop = ['budget', 'revenue', 'bom_budget', 'worldwide_total', 'scrape_success']\n\nprint(f\"\\nDropping {len(columns_to_drop)} redundant columns:\")\nfor col in columns_to_drop:\n    print(f\"  - {col}\")\n\ndf_merged = df_merged.drop(columns=columns_to_drop)\n\nprint(f\"\\nAfter dropping: {len(df_merged.columns)} columns\")\n\n\n# Step 2: Rename final columns for clarity\nprint(\"\\n\" + \"=\"*60)\nprint(\"COLUMN RENAMING\")\nprint(\"=\"*60)\n\nrename_map = {\n    'budget_final': 'budget',\n    'revenue_final': 'revenue_worldwide'\n}\n\nprint(f\"\\nRenaming {len(rename_map)} columns:\")\nfor old, new in rename_map.items():\n    print(f\"  - {old} → {new}\")\n\ndf_merged = df_merged.rename(columns=rename_map)\n\n\n# Step 3: Create new empty columns for future data collection\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING NEW EMPTY COLUMNS\")\nprint(\"=\"*60)\n\n# Theater data columns\nprint(\"\\nTheater data columns (3):\")\ndf_merged['opening_theaters'] = pd.NA\ndf_merged['max_theaters'] = pd.NA\ndf_merged['is_wide_release'] = pd.NA\nprint(\"  - opening_theaters (int)\")\nprint(\"  - max_theaters (int)\")\nprint(\"  - is_wide_release (bool)\")\n\n# Franchise/sequel columns\nprint(\"\\nFranchise feature columns (4):\")\ndf_merged['is_sequel'] = pd.NA\ndf_merged['franchise_name'] = pd.NA\ndf_merged['franchise_number'] = pd.NA\ndf_merged['years_since_last_installment'] = pd.NA\nprint(\"  - is_sequel (bool)\")\nprint(\"  - franchise_name (str)\")\nprint(\"  - franchise_number (int)\")\nprint(\"  - years_since_last_installment (float)\")\n\n# Trailer metrics columns\nprint(\"\\nTrailer metrics columns (3):\")\ndf_merged['trailer_views'] = pd.NA\ndf_merged['trailer_release_date'] = pd.NA\ndf_merged['days_since_trailer'] = pd.NA\nprint(\"  - trailer_views (int)\")\nprint(\"  - trailer_release_date (datetime)\")\nprint(\"  - days_since_trailer (int)\")\n\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"CLEANUP SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nFinal shape: {df_merged.shape}\")\nprint(f\"Final columns: {len(df_merged.columns)}\")\nprint(f\"\\nColumn changes:\")\nprint(f\"  - Dropped: {len(columns_to_drop)} columns\")\nprint(f\"  - Renamed: {len(rename_map)} columns\")\nprint(f\"  - Added: 10 new empty columns\")\nprint(f\"  - Net change: {len(df_merged.columns) - 37} columns (37 → {len(df_merged.columns)})\")\n\nprint(\"\\n✅ Column cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Dataset Column Cleanup and Structure Preparation\n\n### Purpose\nClean up redundant columns and prepare dataset structure for future feature collection.\n\n### Actions\n1. **Drop redundant columns** (5 columns):\n   - `budget`, `revenue`, `bom_budget`, `worldwide_total` → superseded by `_final` versions\n   - `scrape_success` → tracking field, 100% success rate, not needed\n\n2. **Rename final columns** for clarity:\n   - `budget_final` → `budget`\n   - `revenue_final` → `revenue_worldwide`\n\n3. **Create empty columns** for upcoming data collection:\n   - Theater data: `opening_theaters`, `max_theaters`, `is_wide_release`\n   - Franchise features: `is_sequel`, `franchise_name`, `franchise_number`, `years_since_last_installment`\n   - Trailer metrics: `trailer_views`, `trailer_release_date`, `days_since_trailer`\n\n### Result\nClean dataset with 33 → 40 columns (drop 5, rename 2, add 11 new)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Merged Dataset\n",
    "\n",
    "Save the combined TMDB + Box Office Mojo dataset for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved to: data/raw/movies_merged.csv\n",
      "File size: 3530.1 KB\n",
      "Total rows: 5100\n",
      "Total columns: 37\n",
      "\n",
      "Column list:\n",
      "  1. tmdb_id\n",
      "  2. imdb_id\n",
      "  3. title\n",
      "  4. original_title\n",
      "  5. release_date\n",
      "  6. us_release_date\n",
      "  7. us_certification\n",
      "  8. budget\n",
      "  9. revenue\n",
      "  10. runtime\n",
      "  11. genres\n",
      "  12. primary_genre\n",
      "  13. num_genres\n",
      "  14. popularity\n",
      "  15. vote_average\n",
      "  16. vote_count\n",
      "  17. director_id\n",
      "  18. director_name\n",
      "  19. cast_ids\n",
      "  20. cast_names\n",
      "  21. production_companies\n",
      "  22. num_production_companies\n",
      "  23. original_language\n",
      "  24. production_countries\n",
      "  25. youtube_trailer_key\n",
      "  26. tagline\n",
      "  27. overview\n",
      "  28. domestic_total\n",
      "  29. opening_weekend\n",
      "  30. worldwide_total\n",
      "  31. international_total\n",
      "  32. bom_budget\n",
      "  33. scrape_success\n",
      "  34. revenue_final\n",
      "  35. budget_final\n",
      "  36. revenue_source\n",
      "  37. budget_source\n",
      "\n",
      "✅ Data collection complete!\n",
      "Next step: Proceed to 02_data_cleaning_eda.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Cell 29\n",
    "# Save merged dataset\n",
    "merged_output = 'data/raw/movies_merged.csv'\n",
    "df_merged.to_csv(merged_output, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to: {merged_output}\")\n",
    "print(f\"File size: {os.path.getsize(merged_output) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_merged)}\")\n",
    "print(f\"Total columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\nColumn list:\")\n",
    "for i, col in enumerate(df_merged.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n✅ Data collection complete!\")\n",
    "print(\"Next step: Proceed to 02_data_cleaning_eda.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}