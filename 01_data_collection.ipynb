{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "## Purpose\n",
    "This notebook handles the collection of raw movie data from multiple sources including:\n",
    "- **TMDB API**: Movie metadata (budget, cast, crew, genres, runtime, release dates)\n",
    "- **Box Office Mojo**: Box office revenue data (opening weekend, total domestic, worldwide)\n",
    "- **OMDb API**: Supplemental metadata and IMDb ratings\n",
    "- **YouTube Data API**: Trailer view counts and engagement metrics\n",
    "\n",
    "## Objectives\n",
    "1. Set up API connections and test endpoints\n",
    "2. Write data collection functions with error handling and rate limiting\n",
    "3. Collect data for 3,000+ movies from 2010-2024\n",
    "4. Merge data sources on IMDb ID\n",
    "5. Save raw datasets to CSV files in `data/raw/` directory\n",
    "6. Perform initial data inspection\n",
    "\n",
    "## Outputs\n",
    "- `data/raw/movies_tmdb_raw.csv`\n",
    "- `data/raw/revenue_boxofficemojo_raw.csv`\n",
    "- `data/raw/trailers_youtube_raw.csv`\n",
    "\n",
    "## Notes\n",
    "- This notebook may take several hours to run due to API rate limits\n",
    "- Once data is collected, subsequent runs should load from saved CSV files\n",
    "- API keys should be stored in a `.env` file (not committed to git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "TMDB_API_KEY = os.getenv('TMDB_API_KEY')\n",
    "OMDB_API_KEY = os.getenv('OMDB_API_KEY')\n",
    "YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"API Keys loaded:\")\n",
    "print(f\"  TMDB: {'✓' if TMDB_API_KEY else '✗'}\")\n",
    "print(f\"  OMDb: {'✓' if OMDB_API_KEY else '✗'}\")\n",
    "print(f\"  YouTube: {'✓' if YOUTUBE_API_KEY else '✗'}\")\n",
    "\n",
    "# Test TMDB API connection\n",
    "print(\"\\nTesting TMDB API connection...\")\n",
    "test_url = f\"https://api.themoviedb.org/3/movie/550?api_key={TMDB_API_KEY}\"\n",
    "try:\n",
    "    response = requests.get(test_url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ TMDB API connection successful!\")\n",
    "        print(f\"  Test movie: {response.json()['title']}\")\n",
    "    else:\n",
    "        print(f\"✗ TMDB API error: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# TMDB API Base URL\n",
    "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "# Rate limiter class to handle TMDB's 40 requests per 10 seconds limit\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls=40, time_period=10):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_period = time_period\n",
    "        self.calls = []\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Remove calls older than time_period\n",
    "        self.calls = [call_time for call_time in self.calls if now - call_time < self.time_period]\n",
    "        \n",
    "        if len(self.calls) >= self.max_calls:\n",
    "            sleep_time = self.time_period - (now - self.calls[0]) + 0.1\n",
    "            print(f\"  Rate limit reached, waiting {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            self.calls = []\n",
    "        \n",
    "        self.calls.append(time.time())\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(max_calls=35, time_period=10)  # Using 35 to be safe\n",
    "\n",
    "def get_popular_movies_by_year(year, pages=5):\n",
    "    \"\"\"\n",
    "    Get popular movies for a specific year using TMDB discover endpoint.\n",
    "    \n",
    "    Args:\n",
    "        year: Release year (e.g., 2020)\n",
    "        pages: Number of pages to fetch (20 movies per page)\n",
    "    \n",
    "    Returns:\n",
    "        List of movie IDs\n",
    "    \"\"\"\n",
    "    movie_ids = []\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        rate_limiter.wait_if_needed()\n",
    "        \n",
    "        url = f\"{TMDB_BASE_URL}/discover/movie\"\n",
    "        params = {\n",
    "            'api_key': TMDB_API_KEY,\n",
    "            'language': 'en-US',\n",
    "            'sort_by': 'popularity.desc',\n",
    "            'primary_release_year': year,\n",
    "            'page': page,\n",
    "            'vote_count.gte': 50  # Minimum votes to ensure it's not obscure\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                movie_ids.extend([movie['id'] for movie in data['results']])\n",
    "            else:\n",
    "                print(f\"  Error fetching page {page} for year {year}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Exception for year {year}, page {page}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return movie_ids\n",
    "\n",
    "def get_movie_details(movie_id):\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific movie.\n",
    "    \n",
    "    Args:\n",
    "        movie_id: TMDB movie ID\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with movie details or None if error\n",
    "    \"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    \n",
    "    url = f\"{TMDB_BASE_URL}/movie/{movie_id}\"\n",
    "    params = {\n",
    "        'api_key': TMDB_API_KEY,\n",
    "        'append_to_response': 'credits,release_dates,videos'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"  Error fetching movie {movie_id}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Exception for movie {movie_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_movie_data(movie_details):\n",
    "    \"\"\"\n",
    "    Extract relevant fields from TMDB movie details.\n",
    "    \n",
    "    Args:\n",
    "        movie_details: Raw JSON response from TMDB\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extracted fields\n",
    "    \"\"\"\n",
    "    if not movie_details:\n",
    "        return None\n",
    "    \n",
    "    # Extract release dates to find US release\n",
    "    us_release_date = None\n",
    "    us_certification = None\n",
    "    if 'release_dates' in movie_details and 'results' in movie_details['release_dates']:\n",
    "        for country_release in movie_details['release_dates']['results']:\n",
    "            if country_release['iso_3166_1'] == 'US':\n",
    "                for release in country_release['release_dates']:\n",
    "                    if release.get('type') in [2, 3]:  # Theatrical release\n",
    "                        us_release_date = release.get('release_date')\n",
    "                        us_certification = release.get('certification')\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    # Extract cast (top 5 actors)\n",
    "    cast = []\n",
    "    if 'credits' in movie_details and 'cast' in movie_details['credits']:\n",
    "        cast = [\n",
    "            {\n",
    "                'id': actor['id'],\n",
    "                'name': actor['name'],\n",
    "                'order': actor['order']\n",
    "            }\n",
    "            for actor in movie_details['credits']['cast'][:5]\n",
    "        ]\n",
    "    \n",
    "    # Extract director and crew\n",
    "    director = None\n",
    "    if 'credits' in movie_details and 'crew' in movie_details['credits']:\n",
    "        for crew_member in movie_details['credits']['crew']:\n",
    "            if crew_member['job'] == 'Director':\n",
    "                director = {\n",
    "                    'id': crew_member['id'],\n",
    "                    'name': crew_member['name']\n",
    "                }\n",
    "                break\n",
    "    \n",
    "    # Extract YouTube trailer key\n",
    "    trailer_key = None\n",
    "    if 'videos' in movie_details and 'results' in movie_details['videos']:\n",
    "        for video in movie_details['videos']['results']:\n",
    "            if video['type'] == 'Trailer' and video['site'] == 'YouTube':\n",
    "                trailer_key = video['key']\n",
    "                break\n",
    "    \n",
    "    # Extract genres\n",
    "    genres = [genre['name'] for genre in movie_details.get('genres', [])]\n",
    "    \n",
    "    # Extract production companies\n",
    "    production_companies = [company['name'] for company in movie_details.get('production_companies', [])]\n",
    "    \n",
    "    return {\n",
    "        'tmdb_id': movie_details.get('id'),\n",
    "        'imdb_id': movie_details.get('imdb_id'),\n",
    "        'title': movie_details.get('title'),\n",
    "        'original_title': movie_details.get('original_title'),\n",
    "        'release_date': movie_details.get('release_date'),\n",
    "        'us_release_date': us_release_date,\n",
    "        'us_certification': us_certification,\n",
    "        'budget': movie_details.get('budget'),\n",
    "        'revenue': movie_details.get('revenue'),  # Note: TMDB revenue often incomplete\n",
    "        'runtime': movie_details.get('runtime'),\n",
    "        'genres': '|'.join(genres) if genres else None,\n",
    "        'primary_genre': genres[0] if genres else None,\n",
    "        'num_genres': len(genres),\n",
    "        'popularity': movie_details.get('popularity'),\n",
    "        'vote_average': movie_details.get('vote_average'),\n",
    "        'vote_count': movie_details.get('vote_count'),\n",
    "        'director_id': director['id'] if director else None,\n",
    "        'director_name': director['name'] if director else None,\n",
    "        'cast_ids': '|'.join([str(actor['id']) for actor in cast]),\n",
    "        'cast_names': '|'.join([actor['name'] for actor in cast]),\n",
    "        'production_companies': '|'.join(production_companies) if production_companies else None,\n",
    "        'num_production_companies': len(production_companies),\n",
    "        'original_language': movie_details.get('original_language'),\n",
    "        'production_countries': '|'.join([country['iso_3166_1'] for country in movie_details.get('production_countries', [])]),\n",
    "        'youtube_trailer_key': trailer_key,\n",
    "        'tagline': movie_details.get('tagline'),\n",
    "        'overview': movie_details.get('overview')\n",
    "    }\n",
    "\n",
    "def collect_movies_for_year_range(start_year, end_year, pages_per_year=5):\n",
    "    \"\"\"\n",
    "    Collect movie data for a range of years.\n",
    "    \n",
    "    Args:\n",
    "        start_year: Starting year (inclusive)\n",
    "        end_year: Ending year (inclusive)\n",
    "        pages_per_year: Number of pages to fetch per year\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collected movie data\n",
    "    \"\"\"\n",
    "    all_movies = []\n",
    "    total_movies = 0\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\n=== Collecting movies for {year} ===\")\n",
    "        \n",
    "        # Get movie IDs for this year\n",
    "        movie_ids = get_popular_movies_by_year(year, pages=pages_per_year)\n",
    "        print(f\"  Found {len(movie_ids)} movie IDs for {year}\")\n",
    "        \n",
    "        # Get details for each movie\n",
    "        year_movies = 0\n",
    "        for i, movie_id in enumerate(movie_ids, 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Progress: {i}/{len(movie_ids)} movies processed for {year}\")\n",
    "            \n",
    "            movie_details = get_movie_details(movie_id)\n",
    "            if movie_details:\n",
    "                extracted_data = extract_movie_data(movie_details)\n",
    "                if extracted_data:\n",
    "                    all_movies.append(extracted_data)\n",
    "                    year_movies += 1\n",
    "        \n",
    "        print(f\"  Collected {year_movies} movies for {year}\")\n",
    "        total_movies += year_movies\n",
    "        print(f\"  Total movies collected so far: {total_movies}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_movies)\n",
    "    return df\n",
    "\n",
    "print(\"Data collection functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Collect TMDB data for movies from 2010-2024\n",
    "# This will take a while due to rate limiting (approximately 2-3 hours)\n",
    "\n",
    "# Set parameters\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2024\n",
    "PAGES_PER_YEAR = 17  # 17 pages x 20 movies = ~340 movies per year x 15 years = ~5,100 movies\n",
    "\n",
    "print(f\"Starting data collection for {START_YEAR}-{END_YEAR}\")\n",
    "print(f\"Fetching {PAGES_PER_YEAR} pages per year (~{PAGES_PER_YEAR * 20} movies/year)\")\n",
    "print(f\"Estimated total movies: ~{(END_YEAR - START_YEAR + 1) * PAGES_PER_YEAR * 20}\")\n",
    "print(f\"This will take approximately 2-3 hours due to API rate limiting.\\n\")\n",
    "\n",
    "# Collect the data\n",
    "start_time = time.time()\n",
    "df_tmdb = collect_movies_for_year_range(START_YEAR, END_YEAR, pages_per_year=PAGES_PER_YEAR)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Data collection complete!\")\n",
    "print(f\"Total movies collected: {len(df_tmdb)}\")\n",
    "print(f\"Time elapsed: {(end_time - start_time) / 60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Create data/raw directory if it doesn't exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'data/raw/movies_tmdb_raw.csv'\n",
    "df_tmdb.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_tmdb)}\")\n",
    "print(f\"Total columns: {len(df_tmdb.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Basic data inspection\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nShape: {df_tmdb.shape}\")\n",
    "print(f\"  Rows (movies): {df_tmdb.shape[0]}\")\n",
    "print(f\"  Columns (features): {df_tmdb.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN NAMES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = df_tmdb.isnull().sum()\n",
    "missing_pct = (missing / len(df_tmdb) * 100).round(1)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASIC STATISTICS (Numeric Columns)\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Movies with budget data: {df_tmdb['budget'].notna().sum()} ({df_tmdb['budget'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero budget: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with revenue data: {df_tmdb['revenue'].notna().sum()} ({df_tmdb['revenue'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero revenue: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with IMDb ID: {df_tmdb['imdb_id'].notna().sum()} ({df_tmdb['imdb_id'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with director: {df_tmdb['director_name'].notna().sum()} ({df_tmdb['director_name'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with cast data: {df_tmdb['cast_names'].notna().sum()} ({df_tmdb['cast_names'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with YouTube trailer: {df_tmdb['youtube_trailer_key'].notna().sum()} ({df_tmdb['youtube_trailer_key'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE MOVIES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb[['title', 'release_date', 'budget', 'revenue', 'primary_genre', 'director_name']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "df_tmdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Box Office Mojo Revenue Scraping\n",
    "\n",
    "### Purpose\n",
    "Scrape box office revenue data from Box Office Mojo to fill gaps in TMDB data. Currently only 37% of movies have both budget and revenue, limiting our dataset size. Box Office Mojo provides comprehensive revenue data including opening weekend, domestic total, international, and worldwide gross.\n",
    "\n",
    "### Strategy\n",
    "- Target all 5,094 movies with IMDb IDs\n",
    "- Use respectful rate limiting (1.5s delays)\n",
    "- Implement checkpointing for resumability\n",
    "- Extract: opening weekend, domestic total, international total, worldwide total, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Load existing TMDB data if not already in memory\n",
    "if 'df_tmdb' not in locals():\n",
    "    print(\"Loading existing TMDB data from CSV...\")\n",
    "    df_tmdb = pd.read_csv('data/raw/movies_tmdb_raw.csv')\n",
    "    print(f\"Loaded {len(df_tmdb)} movies\")\n",
    "else:\n",
    "    print(f\"df_tmdb already in memory with {len(df_tmdb)} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "def parse_bom_revenue(soup, imdb_id):\n",
    "    \"\"\"\n",
    "    Extract revenue from Box Office Mojo HTML using span.money tags.\n",
    "\n",
    "    BOM structure: Revenue values are in <span class=\"money\"> tags.\n",
    "    We find all money spans and match them to labels by proximity.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object of page HTML\n",
    "        imdb_id: IMDb ID for result dictionary\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with revenue fields or None values if not found\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': True,\n",
    "        'error_message': None\n",
    "    }\n",
    "\n",
    "    # Find all <span class=\"money\"> elements\n",
    "    money_spans = soup.find_all('span', class_='money')\n",
    "\n",
    "    # For each money span, check surrounding context for labels\n",
    "    for money_span in money_spans:\n",
    "        # Get the dollar amount\n",
    "        amount_text = money_span.get_text(strip=True)\n",
    "        if not amount_text or amount_text == '–':\n",
    "            continue\n",
    "\n",
    "        amount = int(amount_text.replace('$', '').replace(',', ''))\n",
    "\n",
    "        # Get parent div to find associated label\n",
    "        parent = money_span.find_parent('div', class_='a-section')\n",
    "        if not parent:\n",
    "            continue\n",
    "\n",
    "        # Get the text of the parent div to find label\n",
    "        parent_text = parent.get_text().lower()\n",
    "\n",
    "        # Match to appropriate field based on label in parent\n",
    "        # Use \"not result[field]\" to only capture the first occurrence\n",
    "        if 'worldwide' in parent_text and result['worldwide_total'] is None:\n",
    "            result['worldwide_total'] = amount\n",
    "        elif 'domestic' in parent_text and 'international' not in parent_text and result['domestic_total'] is None:\n",
    "            result['domestic_total'] = amount\n",
    "        elif 'international' in parent_text and result['international_total'] is None:\n",
    "            result['international_total'] = amount\n",
    "        elif 'opening' in parent_text and result['opening_weekend'] is None:\n",
    "            result['opening_weekend'] = amount\n",
    "        elif 'budget' in parent_text and result['bom_budget'] is None:\n",
    "            result['bom_budget'] = amount\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"HTML parsing function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "def clean_currency(currency_str):\n",
    "    \"\"\"Helper function to convert currency string to integer.\"\"\"\n",
    "    return int(currency_str.replace('$', '').replace(',', ''))\n",
    "\n",
    "def error_result(imdb_id, error_type):\n",
    "    \"\"\"Helper function to create error result dictionary.\"\"\"\n",
    "    return {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': False,\n",
    "        'error_message': error_type\n",
    "    }\n",
    "\n",
    "class BOMRateLimiter:\n",
    "    \"\"\"Rate limiter for Box Office Mojo scraping.\"\"\"\n",
    "    def __init__(self, delay=1.5):\n",
    "        self.delay = delay\n",
    "        self.last_call = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        elapsed = time.time() - self.last_call\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_call = time.time()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "def scrape_bom_movie(imdb_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrape revenue data for a single movie from Box Office Mojo.\n",
    "    \n",
    "    Handles various error conditions:\n",
    "    - 404: Movie not found in BOM\n",
    "    - 429: Rate limited (exponential backoff)\n",
    "    - 5xx: Server errors (retry with delays)\n",
    "    - Timeout: Network timeout (retry once)\n",
    "    - Other exceptions: Catch and log\n",
    "    \n",
    "    Args:\n",
    "        imdb_id: IMDb ID (e.g., 'tt1375666')\n",
    "        max_retries: Maximum retry attempts for recoverable errors\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with revenue data or error information\n",
    "    \"\"\"\n",
    "    url = f\"https://www.boxofficemojo.com/title/{imdb_id}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; MovieDataCollector/1.0)'}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "            if response.status_code == 404:\n",
    "                # Movie not in Box Office Mojo database\n",
    "                return error_result(imdb_id, 'not_found')\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited - wait with exponential backoff\n",
    "                wait = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                print(f\"  Rate limited for {imdb_id}, waiting {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            elif response.status_code >= 500:\n",
    "                # Server error - retry if attempts remain\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  Server error {response.status_code} for {imdb_id}, retrying...\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                return error_result(imdb_id, f'server_error_{response.status_code}')\n",
    "\n",
    "            elif response.status_code == 200:\n",
    "                # Success - parse the HTML\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                return parse_bom_revenue(soup, imdb_id)\n",
    "\n",
    "            else:\n",
    "                # Unexpected status code\n",
    "                return error_result(imdb_id, f'http_{response.status_code}')\n",
    "\n",
    "        except requests.Timeout:\n",
    "            # Network timeout - retry if attempts remain\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  Timeout for {imdb_id}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return error_result(imdb_id, 'timeout')\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch-all for unexpected errors\n",
    "            error_msg = str(e)[:50]  # Truncate long error messages\n",
    "            return error_result(imdb_id, f'exception_{error_msg}')\n",
    "\n",
    "    # Max retries exhausted\n",
    "    return error_result(imdb_id, 'max_retries')\n",
    "\n",
    "print(\"Main scraping function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "def scrape_bom_batch(imdb_ids, checkpoint_file='data/raw/bom_checkpoint.csv', save_every=100):\n",
    "    \"\"\"\n",
    "    Scrape multiple movies with automatic checkpointing for resumability.\n",
    "    \n",
    "    If interrupted, the function can resume from the last checkpoint by\n",
    "    simply running again - it will load completed IMDb IDs and skip them.\n",
    "    \n",
    "    Args:\n",
    "        imdb_ids: List of IMDb IDs to scrape\n",
    "        checkpoint_file: Path to save progress (CSV format)\n",
    "        save_every: Save checkpoint every N movies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all scraped results\n",
    "    \"\"\"\n",
    "    # Load existing checkpoint if available\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "        completed = set(df_checkpoint['imdb_id'].dropna())\n",
    "        results = df_checkpoint.to_dict('records')\n",
    "        print(f\"Resuming from checkpoint: {len(completed)} already scraped\")\n",
    "    else:\n",
    "        completed = set()\n",
    "        results = []\n",
    "        print(\"Starting fresh scrape (no checkpoint found)\")\n",
    "\n",
    "    # Filter to unscraped movies\n",
    "    remaining = [id for id in imdb_ids if id not in completed]\n",
    "    print(f\"Scraping {len(remaining)} movies...\")\n",
    "    print(f\"Estimated time: {len(remaining) * 2 / 3600:.1f} hours\\n\")\n",
    "\n",
    "    # Initialize rate limiter\n",
    "    limiter = BOMRateLimiter(delay=1.5)\n",
    "\n",
    "    # Scrape each movie\n",
    "    for i, imdb_id in enumerate(remaining, 1):\n",
    "        limiter.wait()  # Respect rate limit before each request\n",
    "\n",
    "        result = scrape_bom_movie(imdb_id)\n",
    "        results.append(result)\n",
    "\n",
    "        # Progress report every 50 movies\n",
    "        if i % 50 == 0:\n",
    "            success = sum(1 for r in results[-i:] if r['scrape_success'])\n",
    "            print(f\"  Progress: {i}/{len(remaining)} | Recent success rate: {success}/{min(i, 50)} ({success/min(i, 50)*100:.1f}%)\")\n",
    "\n",
    "        # Save checkpoint every N movies\n",
    "        if i % save_every == 0:\n",
    "            pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "            print(f\"  Checkpoint saved ({len(results)} total movies)\")\n",
    "\n",
    "    # Final save\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(checkpoint_file, index=False)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    total_success = df['scrape_success'].sum()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping complete!\")\n",
    "    print(f\"  Total movies: {len(df)}\")\n",
    "    print(f\"  Successful: {total_success} ({total_success/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Failed: {len(df) - total_success}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Batch processing function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Scraping on Sample Movies\n",
    "\n",
    "Before running the full 3-4 hour scraping job, test on a few movies to verify the scraping logic works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 - TEST SCRAPING ON 5 MOVIES\n",
    "# Test scraper on 5 sample movies\n",
    "print(\"Testing Box Office Mojo scraper on sample movies...\\n\")\n",
    "\n",
    "test_ids = df_tmdb.head(5)['imdb_id'].dropna().tolist()\n",
    "\n",
    "for test_id in test_ids:\n",
    "    result = scrape_bom_movie(test_id)\n",
    "    \n",
    "    # Get movie title for context\n",
    "    title = df_tmdb[df_tmdb['imdb_id'] == test_id]['title'].values[0]\n",
    "    \n",
    "    print(f\"{test_id} ({title}):\")\n",
    "    print(f\"  Success: {result['scrape_success']}\")\n",
    "    print(f\"  Worldwide: ${result['worldwide_total']:,}\" if result['worldwide_total'] else f\"  Worldwide: None\")\n",
    "    print(f\"  Domestic: ${result['domestic_total']:,}\" if result['domestic_total'] else f\"  Domestic: None\")\n",
    "    print(f\"  Opening: ${result['opening_weekend']:,}\" if result['opening_weekend'] else f\"  Opening: None\")\n",
    "    if result['error_message']:\n",
    "        print(f\"  Error: {result['error_message']}\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(1.5)  # Rate limit during test\n",
    "\n",
    "print(\"Test complete! If results look good, proceed to full scraping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Full Box Office Mojo Scraping\n",
    "\n",
    "**Note:** This will take approximately 3-4 hours. The scraper uses checkpointing, so it can be safely interrupted and resumed. Consider running overnight or during a long break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Box Office Mojo scraping...\n",
      "Total movies to scrape: 5094\n",
      "Estimated time: 2.8 hours\n",
      "Started at: 2026-01-21 14:03:55\n",
      "\n",
      "Resuming from checkpoint: 3400 already scraped\n",
      "Scraping 1694 movies...\n",
      "Estimated time: 0.9 hours\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run batch scraper with checkpointing\u001b[39;00m\n\u001b[32m     11\u001b[39m scrape_start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_bom = \u001b[43mscrape_bom_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimdb_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/raw/bom_checkpoint.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m scrape_end = time.time()\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinished at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mscrape_bom_batch\u001b[39m\u001b[34m(imdb_ids, checkpoint_file, save_every)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, imdb_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(remaining, \u001b[32m1\u001b[39m):\n\u001b[32m     38\u001b[39m     limiter.wait()  \u001b[38;5;66;03m# Respect rate limit before each request\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     result = \u001b[43mscrape_bom_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     results.append(result)\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Progress report every 50 movies\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mscrape_bom_movie\u001b[39m\u001b[34m(imdb_id, max_retries)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Success - parse the HTML\u001b[39;00m\n\u001b[32m     48\u001b[39m     soup = BeautifulSoup(response.content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_bom_revenue\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Unexpected status code\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m error_result(imdb_id, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mparse_bom_revenue\u001b[39m\u001b[34m(soup, imdb_id)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Get parent div to find associated label\u001b[39;00m\n\u001b[32m     40\u001b[39m parent = money_span.find_parent(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33ma-section\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Get the text of the parent div to find label\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FOLDERS/Data_Science_Projects/Movie_Box_Office_Success/venv/lib/python3.14/site-packages/bs4/element.py:2414\u001b[39m, in \u001b[36mTag.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.contents\n\u001b[32m-> \u001b[39m\u001b[32m2414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2415\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mA tag is non-None even if it has no contents.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 21\n",
    "# Get all IMDb IDs from TMDB data\n",
    "imdb_ids = df_tmdb['imdb_id'].dropna().tolist()\n",
    "\n",
    "print(f\"Starting Box Office Mojo scraping...\")\n",
    "print(f\"Total movies to scrape: {len(imdb_ids)}\")\n",
    "print(f\"Estimated time: {len(imdb_ids) * 2 / 3600:.1f} hours\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Run batch scraper with checkpointing\n",
    "scrape_start = time.time()\n",
    "df_bom = scrape_bom_batch(\n",
    "    imdb_ids,\n",
    "    checkpoint_file='data/raw/bom_checkpoint.csv',\n",
    "    save_every=100\n",
    ")\n",
    "scrape_end = time.time()\n",
    "\n",
    "print(f\"\\nFinished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {(scrape_end - scrape_start) / 60:.1f} minutes ({(scrape_end - scrape_start) / 3600:.2f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Raw Box Office Mojo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23\n",
    "# Save Box Office Mojo data as raw CSV\n",
    "output_file = 'data/raw/revenue_boxofficemojo_raw.csv'\n",
    "df_bom.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Box Office Mojo data saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_bom)}\")\n",
    "print(f\"Total columns: {len(df_bom.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_bom.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Merge TMDB and Box Office Mojo Data\n",
    "\n",
    "Combine the two data sources, preferring BOM revenue (more complete) over TMDB revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25\n",
    "# Merge TMDB and BOM data on IMDb ID\n",
    "df_merged = df_tmdb.merge(\n",
    "    df_bom[['imdb_id', 'domestic_total', 'opening_weekend', 'worldwide_total', 'international_total', 'bom_budget', 'scrape_success']],\n",
    "    on='imdb_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create final revenue column - prefer BOM worldwide, fallback to TMDB revenue\n",
    "df_merged['revenue_final'] = df_merged['worldwide_total'].fillna(df_merged['revenue'])\n",
    "\n",
    "# Create final budget column - prefer BOM budget, fallback to TMDB budget\n",
    "df_merged['budget_final'] = df_merged['bom_budget'].fillna(df_merged['budget'])\n",
    "\n",
    "# Track revenue source for transparency\n",
    "df_merged['revenue_source'] = 'none'\n",
    "df_merged.loc[df_merged['revenue'] > 0, 'revenue_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['worldwide_total'].notna(), 'revenue_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna()), 'revenue_source'] = 'both'\n",
    "\n",
    "# Track budget source for transparency\n",
    "df_merged['budget_source'] = 'none'\n",
    "df_merged.loc[df_merged['budget'] > 0, 'budget_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['bom_budget'].notna(), 'budget_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['budget'] > 0) & (df_merged['bom_budget'].notna()), 'budget_source'] = 'both'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MERGE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"  Rows: {len(df_merged)}\")\n",
    "print(f\"  Columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\nRevenue source breakdown:\")\n",
    "print(df_merged['revenue_source'].value_counts())\n",
    "\n",
    "print(\"\\nBudget source breakdown:\")\n",
    "print(df_merged['budget_source'].value_counts())\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "print(\"  - domestic_total (from BOM)\")\n",
    "print(\"  - opening_weekend (from BOM)\")\n",
    "print(\"  - worldwide_total (from BOM)\")\n",
    "print(\"  - international_total (from BOM)\")\n",
    "print(\"  - bom_budget (from BOM)\")\n",
    "print(\"  - revenue_final (combined best source)\")\n",
    "print(\"  - budget_final (combined best source)\")\n",
    "print(\"  - revenue_source (tracking field)\")\n",
    "print(\"  - budget_source (tracking field)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quality Analysis\n",
    "\n",
    "Analyze scraping results, gap filling, and dataset completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27\n",
    "print(\"=\"*60)\n",
    "print(\"BOX OFFICE MOJO SCRAPING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scraping success rate\n",
    "total_scraped = len(df_bom)\n",
    "successful = df_bom['scrape_success'].sum()\n",
    "print(f\"\\nScraping Success Rate:\")\n",
    "print(f\"  Total attempted: {total_scraped}\")\n",
    "print(f\"  Successful: {successful} ({successful/total_scraped*100:.1f}%)\")\n",
    "print(f\"  Failed: {total_scraped - successful}\")\n",
    "\n",
    "# Most common errors\n",
    "if (total_scraped - successful) > 0:\n",
    "    print(\"\\nMost common errors:\")\n",
    "    error_counts = df_bom[~df_bom['scrape_success']]['error_message'].value_counts().head(5)\n",
    "    for error, count in error_counts.items():\n",
    "        print(f\"  {error}: {count}\")\n",
    "\n",
    "# Revenue coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB revenue > 0: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM revenue available: {df_merged['worldwide_total'].notna().sum()} ({df_merged['worldwide_total'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final revenue > 0: {(df_merged['revenue_final'] > 0).sum()} ({(df_merged['revenue_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for revenue\n",
    "tmdb_missing = (df_merged['revenue'] == 0) | (df_merged['revenue'].isna())\n",
    "bom_filled = df_merged['worldwide_total'].notna()\n",
    "gaps_filled = (tmdb_missing & bom_filled).sum()\n",
    "\n",
    "print(f\"\\nRevenue Gap Filling:\")\n",
    "print(f\"  TMDB revenue gaps: {tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {gaps_filled/tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Budget coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BUDGET COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB budget > 0: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM budget available: {df_merged['bom_budget'].notna().sum()} ({df_merged['bom_budget'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final budget > 0: {(df_merged['budget_final'] > 0).sum()} ({(df_merged['budget_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for budget\n",
    "budget_tmdb_missing = (df_merged['budget'] == 0) | (df_merged['budget'].isna())\n",
    "budget_bom_filled = df_merged['bom_budget'].notna()\n",
    "budget_gaps_filled = (budget_tmdb_missing & budget_bom_filled).sum()\n",
    "\n",
    "print(f\"\\nBudget Gap Filling:\")\n",
    "print(f\"  TMDB budget gaps: {budget_tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {budget_gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {budget_gaps_filled/budget_tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Revenue comparison for movies with both sources\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COMPARISON (Movies with Both Sources)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "both = (df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna())\n",
    "if both.sum() > 0:\n",
    "    df_compare = df_merged[both].copy()\n",
    "    df_compare['diff'] = abs(df_compare['revenue'] - df_compare['worldwide_total'])\n",
    "    df_compare['diff_pct'] = df_compare['diff'] / df_compare['revenue'] * 100\n",
    "\n",
    "    print(f\"\\nCount: {len(df_compare)}\")\n",
    "    print(f\"Mean absolute difference: ${df_compare['diff'].mean():,.0f}\")\n",
    "    print(f\"Median absolute difference: ${df_compare['diff'].median():,.0f}\")\n",
    "    print(f\"Mean % difference: {df_compare['diff_pct'].mean():.1f}%\")\n",
    "    print(f\"Median % difference: {df_compare['diff_pct'].median():.1f}%\")\n",
    "    print(f\"\\nMovies with >20% difference: {(df_compare['diff_pct'] > 20).sum()} ({(df_compare['diff_pct'] > 20).sum()/len(df_compare)*100:.1f}%)\")\n",
    "    \n",
    "    # Show a few examples of large discrepancies\n",
    "    if (df_compare['diff_pct'] > 20).sum() > 0:\n",
    "        print(\"\\nExample large discrepancies:\")\n",
    "        large_diff = df_compare.nlargest(3, 'diff_pct')[['title', 'revenue', 'worldwide_total', 'diff_pct']]\n",
    "        for idx, row in large_diff.iterrows():\n",
    "            print(f\"  {row['title']}: TMDB=${row['revenue']:,} vs BOM=${row['worldwide_total']:,} ({row['diff_pct']:.1f}% diff)\")\n",
    "\n",
    "# Dataset readiness check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READINESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Before BOM\n",
    "complete_before = (df_tmdb['budget'] > 0) & (df_tmdb['revenue'] > 0)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  Movies with budget > 0: {(df_tmdb['budget'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue > 0: {(df_tmdb['revenue'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_before.sum()} ({complete_before.sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "# After BOM\n",
    "complete_after = (df_merged['budget_final'] > 0) & (df_merged['revenue_final'] > 0)\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  Movies with budget_final > 0: {(df_merged['budget_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue_final > 0: {(df_merged['revenue_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_after.sum()} ({complete_after.sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "improvement = complete_after.sum() - complete_before.sum()\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Additional complete movies: +{improvement}\")\n",
    "print(f\"  Improvement rate: +{improvement/complete_before.sum()*100:.1f}%\")\n",
    "\n",
    "# Target assessment\n",
    "TARGET = 2500\n",
    "print(f\"\\nTarget Assessment (2,500-3,000 movies needed):\")\n",
    "if complete_after.sum() >= 3000:\n",
    "    print(f\"  ✅ EXCEEDS upper target! ({complete_after.sum():,} complete movies)\")\n",
    "elif complete_after.sum() >= TARGET:\n",
    "    print(f\"  ✅ MEETS lower target! ({complete_after.sum():,} complete movies)\")\n",
    "else:\n",
    "    shortfall = TARGET - complete_after.sum()\n",
    "    print(f\"  ⚠️  Short by {shortfall:,} movies (have {complete_after.sum():,}, need {TARGET:,})\")\n",
    "    print(f\"  Completion: {complete_after.sum()/TARGET*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Merged Dataset\n",
    "\n",
    "Save the combined TMDB + Box Office Mojo dataset for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29\n",
    "# Save merged dataset\n",
    "merged_output = 'data/raw/movies_merged.csv'\n",
    "df_merged.to_csv(merged_output, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to: {merged_output}\")\n",
    "print(f\"File size: {os.path.getsize(merged_output) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_merged)}\")\n",
    "print(f\"Total columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\nColumn list:\")\n",
    "for i, col in enumerate(df_merged.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n✅ Data collection complete!\")\n",
    "print(\"Next step: Proceed to 02_data_cleaning_eda.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
