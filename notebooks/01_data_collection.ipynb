{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "## Purpose\n",
    "This notebook handles the collection of raw movie data from multiple sources including:\n",
    "- **TMDB API**: Movie metadata (budget, cast, crew, genres, runtime, release dates)\n",
    "- **Box Office Mojo**: Box office revenue data (opening weekend, total domestic, worldwide)\n",
    "- **OMDb API**: Supplemental metadata and IMDb ratings\n",
    "- **YouTube Data API**: Trailer view counts and engagement metrics\n",
    "\n",
    "## Objectives\n",
    "1. Set up API connections and test endpoints\n",
    "2. Write data collection functions with error handling and rate limiting\n",
    "3. Collect data for 3,000+ movies from 2010-2024\n",
    "4. Merge data sources on IMDb ID\n",
    "5. Save raw datasets to CSV files in `data/raw/` directory\n",
    "6. Perform initial data inspection\n",
    "\n",
    "## Outputs\n",
    "- `data/raw/movies_tmdb_raw.csv`\n",
    "- `data/raw/revenue_boxofficemojo_raw.csv`\n",
    "- `data/raw/trailers_youtube_raw.csv`\n",
    "\n",
    "## Notes\n",
    "- This notebook may take several hours to run due to API rate limits\n",
    "- Once data is collected, subsequent runs should load from saved CSV files\n",
    "- API keys should be stored in a `.env` file (not committed to git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys loaded:\n",
      "  TMDB: ✓\n",
      "  OMDb: ✓\n",
      "  YouTube: ✓\n",
      "\n",
      "Testing TMDB API connection...\n",
      "✓ TMDB API connection successful!\n",
      "  Test movie: Fight Club\n"
     ]
    }
   ],
   "source": [
    "# ========== Cell 1: Import Libraries & Test API Connections ==========\n",
    "# Imports all required libraries and loads API keys from .env file. Tests TMDB API connection.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "TMDB_API_KEY = os.getenv('TMDB_API_KEY')\n",
    "OMDB_API_KEY = os.getenv('OMDB_API_KEY')\n",
    "YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"API Keys loaded:\")\n",
    "print(f\"  TMDB: {'✓' if TMDB_API_KEY else '✗'}\")\n",
    "print(f\"  OMDb: {'✓' if OMDB_API_KEY else '✗'}\")\n",
    "print(f\"  YouTube: {'✓' if YOUTUBE_API_KEY else '✗'}\")\n",
    "\n",
    "# Test TMDB API connection\n",
    "print(\"\\nTesting TMDB API connection...\")\n",
    "test_url = f\"https://api.themoviedb.org/3/movie/550?api_key={TMDB_API_KEY}\"\n",
    "try:\n",
    "    response = requests.get(test_url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ TMDB API connection successful!\")\n",
    "        print(f\"  Test movie: {response.json()['title']}\")\n",
    "    else:\n",
    "        print(f\"✗ TMDB API error: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 2: Define TMDB Collection Functions ==========\n",
    "# Defines RateLimiter class and helper functions for TMDB API data collection.\n",
    "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "# Rate limiter class to handle TMDB's 40 requests per 10 seconds limit\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls=40, time_period=10):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_period = time_period\n",
    "        self.calls = []\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Remove calls older than time_period\n",
    "        self.calls = [call_time for call_time in self.calls if now - call_time < self.time_period]\n",
    "        \n",
    "        if len(self.calls) >= self.max_calls:\n",
    "            sleep_time = self.time_period - (now - self.calls[0]) + 0.1\n",
    "            print(f\"  Rate limit reached, waiting {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            self.calls = []\n",
    "        \n",
    "        self.calls.append(time.time())\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(max_calls=35, time_period=10)  # Using 35 to be safe\n",
    "\n",
    "def get_popular_movies_by_year(year, pages=5):\n",
    "    \"\"\"\n",
    "    Get popular movies for a specific year using TMDB discover endpoint.\n",
    "    \n",
    "    Args:\n",
    "        year: Release year (e.g., 2020)\n",
    "        pages: Number of pages to fetch (20 movies per page)\n",
    "    \n",
    "    Returns:\n",
    "        List of movie IDs\n",
    "    \"\"\"\n",
    "    movie_ids = []\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        rate_limiter.wait_if_needed()\n",
    "        \n",
    "        url = f\"{TMDB_BASE_URL}/discover/movie\"\n",
    "        params = {\n",
    "            'api_key': TMDB_API_KEY,\n",
    "            'language': 'en-US',\n",
    "            'sort_by': 'popularity.desc',\n",
    "            'primary_release_year': year,\n",
    "            'page': page,\n",
    "            'vote_count.gte': 50  # Minimum votes to ensure it's not obscure\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                movie_ids.extend([movie['id'] for movie in data['results']])\n",
    "            else:\n",
    "                print(f\"  Error fetching page {page} for year {year}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Exception for year {year}, page {page}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return movie_ids\n",
    "\n",
    "def get_movie_details(movie_id):\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific movie.\n",
    "    \n",
    "    Args:\n",
    "        movie_id: TMDB movie ID\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with movie details or None if error\n",
    "    \"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    \n",
    "    url = f\"{TMDB_BASE_URL}/movie/{movie_id}\"\n",
    "    params = {\n",
    "        'api_key': TMDB_API_KEY,\n",
    "        'append_to_response': 'credits,release_dates,videos'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"  Error fetching movie {movie_id}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Exception for movie {movie_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_movie_data(movie_details):\n",
    "    \"\"\"\n",
    "    Extract relevant fields from TMDB movie details.\n",
    "    \n",
    "    Args:\n",
    "        movie_details: Raw JSON response from TMDB\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extracted fields\n",
    "    \"\"\"\n",
    "    if not movie_details:\n",
    "        return None\n",
    "    \n",
    "    # Extract release dates to find US release\n",
    "    us_release_date = None\n",
    "    us_certification = None\n",
    "    if 'release_dates' in movie_details and 'results' in movie_details['release_dates']:\n",
    "        for country_release in movie_details['release_dates']['results']:\n",
    "            if country_release['iso_3166_1'] == 'US':\n",
    "                for release in country_release['release_dates']:\n",
    "                    if release.get('type') in [2, 3]:  # Theatrical release\n",
    "                        us_release_date = release.get('release_date')\n",
    "                        us_certification = release.get('certification')\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    # Extract cast (top 5 actors)\n",
    "    cast = []\n",
    "    if 'credits' in movie_details and 'cast' in movie_details['credits']:\n",
    "        cast = [\n",
    "            {\n",
    "                'id': actor['id'],\n",
    "                'name': actor['name'],\n",
    "                'order': actor['order']\n",
    "            }\n",
    "            for actor in movie_details['credits']['cast'][:5]\n",
    "        ]\n",
    "    \n",
    "    # Extract director and crew\n",
    "    director = None\n",
    "    if 'credits' in movie_details and 'crew' in movie_details['credits']:\n",
    "        for crew_member in movie_details['credits']['crew']:\n",
    "            if crew_member['job'] == 'Director':\n",
    "                director = {\n",
    "                    'id': crew_member['id'],\n",
    "                    'name': crew_member['name']\n",
    "                }\n",
    "                break\n",
    "    \n",
    "    # Extract YouTube trailer key\n",
    "    trailer_key = None\n",
    "    if 'videos' in movie_details and 'results' in movie_details['videos']:\n",
    "        for video in movie_details['videos']['results']:\n",
    "            if video['type'] == 'Trailer' and video['site'] == 'YouTube':\n",
    "                trailer_key = video['key']\n",
    "                break\n",
    "    \n",
    "    # Extract genres\n",
    "    genres = [genre['name'] for genre in movie_details.get('genres', [])]\n",
    "    \n",
    "    # Extract production companies\n",
    "    production_companies = [company['name'] for company in movie_details.get('production_companies', [])]\n",
    "    \n",
    "    return {\n",
    "        'tmdb_id': movie_details.get('id'),\n",
    "        'imdb_id': movie_details.get('imdb_id'),\n",
    "        'title': movie_details.get('title'),\n",
    "        'original_title': movie_details.get('original_title'),\n",
    "        'release_date': movie_details.get('release_date'),\n",
    "        'us_release_date': us_release_date,\n",
    "        'us_certification': us_certification,\n",
    "        'budget': movie_details.get('budget'),\n",
    "        'revenue': movie_details.get('revenue'),  # Note: TMDB revenue often incomplete\n",
    "        'runtime': movie_details.get('runtime'),\n",
    "        'genres': '|'.join(genres) if genres else None,\n",
    "        'primary_genre': genres[0] if genres else None,\n",
    "        'num_genres': len(genres),\n",
    "        'popularity': movie_details.get('popularity'),\n",
    "        'vote_average': movie_details.get('vote_average'),\n",
    "        'vote_count': movie_details.get('vote_count'),\n",
    "        'director_id': director['id'] if director else None,\n",
    "        'director_name': director['name'] if director else None,\n",
    "        'cast_ids': '|'.join([str(actor['id']) for actor in cast]),\n",
    "        'cast_names': '|'.join([actor['name'] for actor in cast]),\n",
    "        'production_companies': '|'.join(production_companies) if production_companies else None,\n",
    "        'num_production_companies': len(production_companies),\n",
    "        'original_language': movie_details.get('original_language'),\n",
    "        'production_countries': '|'.join([country['iso_3166_1'] for country in movie_details.get('production_countries', [])]),\n",
    "        'youtube_trailer_key': trailer_key,\n",
    "        'tagline': movie_details.get('tagline'),\n",
    "        'overview': movie_details.get('overview')\n",
    "    }\n",
    "\n",
    "def collect_movies_for_year_range(start_year, end_year, pages_per_year=5):\n",
    "    \"\"\"\n",
    "    Collect movie data for a range of years.\n",
    "    \n",
    "    Args:\n",
    "        start_year: Starting year (inclusive)\n",
    "        end_year: Ending year (inclusive)\n",
    "        pages_per_year: Number of pages to fetch per year\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collected movie data\n",
    "    \"\"\"\n",
    "    all_movies = []\n",
    "    total_movies = 0\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\n=== Collecting movies for {year} ===\")\n",
    "        \n",
    "        # Get movie IDs for this year\n",
    "        movie_ids = get_popular_movies_by_year(year, pages=pages_per_year)\n",
    "        print(f\"  Found {len(movie_ids)} movie IDs for {year}\")\n",
    "        \n",
    "        # Get details for each movie\n",
    "        year_movies = 0\n",
    "        for i, movie_id in enumerate(movie_ids, 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Progress: {i}/{len(movie_ids)} movies processed for {year}\")\n",
    "            \n",
    "            movie_details = get_movie_details(movie_id)\n",
    "            if movie_details:\n",
    "                extracted_data = extract_movie_data(movie_details)\n",
    "                if extracted_data:\n",
    "                    all_movies.append(extracted_data)\n",
    "                    year_movies += 1\n",
    "        \n",
    "        print(f\"  Collected {year_movies} movies for {year}\")\n",
    "        total_movies += year_movies\n",
    "        print(f\"  Total movies collected so far: {total_movies}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_movies)\n",
    "    return df\n",
    "\n",
    "print(\"Data collection functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 3: Define OMDb Functions (Not Used) ==========\n",
    "# Defines OMDb API functions; confirms OMDb doesn't provide budget data.\n",
    "\n",
    "class OMDbRateLimiter:\n",
    "    \"\"\"Rate limiter for OMDb API (1,000 requests per day limit).\"\"\"\n",
    "    def __init__(self, max_calls_per_day=1000):\n",
    "        self.max_calls_per_day = max_calls_per_day\n",
    "        self.calls_today = 0\n",
    "        self.last_reset = datetime.now().date()\n",
    "        self.min_delay = 1.0  # Minimum 1 second between requests\n",
    "        self.last_call = 0\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Wait if rate limit reached or if minimum delay not elapsed.\"\"\"\n",
    "        # Check if we need to reset daily counter\n",
    "        today = datetime.now().date()\n",
    "        if today > self.last_reset:\n",
    "            self.calls_today = 0\n",
    "            self.last_reset = today\n",
    "            print(f\"  Daily rate limit reset. New day: {today}\")\n",
    "        \n",
    "        # Check daily limit\n",
    "        if self.calls_today >= self.max_calls_per_day:\n",
    "            # Calculate time until midnight\n",
    "            now = datetime.now()\n",
    "            tomorrow = datetime.combine(today + pd.Timedelta(days=1), datetime.min.time())\n",
    "            wait_seconds = (tomorrow - now).total_seconds()\n",
    "            print(f\"  Daily rate limit reached ({self.max_calls_per_day} requests).\")\n",
    "            print(f\"  Waiting until midnight ({wait_seconds/3600:.1f} hours)...\")\n",
    "            time.sleep(wait_seconds + 1)\n",
    "            self.calls_today = 0\n",
    "            self.last_reset = datetime.now().date()\n",
    "        \n",
    "        # Enforce minimum delay between requests\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_call\n",
    "        if elapsed < self.min_delay:\n",
    "            time.sleep(self.min_delay - elapsed)\n",
    "        \n",
    "        self.last_call = time.time()\n",
    "        self.calls_today += 1\n",
    "\n",
    "\n",
    "def parse_omdb_budget(budget_str):\n",
    "    \"\"\"\n",
    "    Parse OMDb budget string to integer.\n",
    "    \n",
    "    Handles formats:\n",
    "    - \"$50,000,000\" → 50000000\n",
    "    - \"$50 million\" → 50000000\n",
    "    - \"50000000\" → 50000000\n",
    "    - \"N/A\" → None\n",
    "    - Missing/empty → None\n",
    "    \n",
    "    Args:\n",
    "        budget_str: Budget string from OMDb API\n",
    "    \n",
    "    Returns:\n",
    "        Integer budget value or None\n",
    "    \"\"\"\n",
    "    if not budget_str or budget_str == 'N/A':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Remove $ and commas\n",
    "        cleaned = budget_str.replace('$', '').replace(',', '').strip()\n",
    "        \n",
    "        # Handle \"X million\" format\n",
    "        if 'million' in cleaned.lower():\n",
    "            number = float(cleaned.lower().replace('million', '').strip())\n",
    "            return int(number * 1_000_000)\n",
    "        \n",
    "        # Handle \"X billion\" format (rare)\n",
    "        if 'billion' in cleaned.lower():\n",
    "            number = float(cleaned.lower().replace('billion', '').strip())\n",
    "            return int(number * 1_000_000_000)\n",
    "        \n",
    "        # Handle plain number\n",
    "        return int(float(cleaned))\n",
    "    \n",
    "    except (ValueError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_omdb_data(imdb_id, api_key):\n",
    "    \"\"\"\n",
    "    Get budget data for a movie from OMDb API.\n",
    "    \n",
    "    Args:\n",
    "        imdb_id: IMDb ID (e.g., 'tt1375666')\n",
    "        api_key: OMDb API key\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with imdb_id, omdb_budget, budget_raw, error\n",
    "    \"\"\"\n",
    "    url = \"http://www.omdbapi.com/\"\n",
    "    params = {\n",
    "        'i': imdb_id,\n",
    "        'apikey': api_key,\n",
    "        'plot': 'short'  # Minimize response size\n",
    "    }\n",
    "    \n",
    "    result = {\n",
    "        'imdb_id': imdb_id,\n",
    "        'omdb_budget': None,\n",
    "        'budget_raw': None,\n",
    "        'omdb_success': False,\n",
    "        'omdb_error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Check if movie was found\n",
    "            if data.get('Response') == 'False':\n",
    "                result['omdb_error'] = data.get('Error', 'not_found')\n",
    "                return result\n",
    "            \n",
    "            # Extract budget\n",
    "            budget_str = data.get('BoxOffice') or data.get('Budget')  # Try BoxOffice first, then Budget field\n",
    "            \n",
    "            # Actually, OMDb doesn't have a \"Budget\" field - it's typically not in their data\n",
    "            # Let me check the actual response structure - OMDb mainly has: BoxOffice (revenue), not budget\n",
    "            # I need to reconsider this - OMDb API doesn't actually provide budget data!\n",
    "            \n",
    "            # NOTE: After checking OMDb API documentation, budget is NOT a standard field\n",
    "            # OMDb primarily provides: Title, Year, Rated, Released, Runtime, Genre, Director, \n",
    "            # Writer, Actors, Plot, Language, Country, Awards, Poster, Ratings, Metascore, \n",
    "            # imdbRating, imdbVotes, imdbID, Type, DVD, BoxOffice (revenue, not budget), \n",
    "            # Production, Website, Response\n",
    "            \n",
    "            # So this function won't work as intended. We need to flag this.\n",
    "            result['omdb_error'] = 'budget_field_not_available'\n",
    "            return result\n",
    "            \n",
    "        elif response.status_code == 401:\n",
    "            result['omdb_error'] = 'invalid_api_key'\n",
    "            return result\n",
    "        elif response.status_code == 429:\n",
    "            result['omdb_error'] = 'rate_limited'\n",
    "            return result\n",
    "        else:\n",
    "            result['omdb_error'] = f'http_{response.status_code}'\n",
    "            return result\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        result['omdb_error'] = 'timeout'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        result['omdb_error'] = f'exception_{str(e)[:30]}'\n",
    "        return result\n",
    "\n",
    "\n",
    "print(\"⚠️  WARNING: OMDb API does NOT provide budget data!\")\n",
    "print(\"The OMDb API documentation shows they provide BoxOffice (revenue) but NOT budget.\")\n",
    "print(\"Budget field is not available in OMDb responses.\")\n",
    "print(\"\\nRecommendation: Skip OMDb for budget collection and proceed directly to Wikipedia scraping.\")\n",
    "print(\"See plan for alternative budget sources: Wikipedia, The Numbers, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 4: Execute TMDB Collection ==========\n",
    "# Collects movie metadata from TMDB API for 2010-2024 (~5,100 movies, takes 2-3 hours).\n",
    "\n",
    "# Set parameters\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2024\n",
    "PAGES_PER_YEAR = 17  # 17 pages x 20 movies = ~340 movies per year x 15 years = ~5,100 movies\n",
    "\n",
    "print(f\"Starting data collection for {START_YEAR}-{END_YEAR}\")\n",
    "print(f\"Fetching {PAGES_PER_YEAR} pages per year (~{PAGES_PER_YEAR * 20} movies/year)\")\n",
    "print(f\"Estimated total movies: ~{(END_YEAR - START_YEAR + 1) * PAGES_PER_YEAR * 20}\")\n",
    "print(f\"This will take approximately 2-3 hours due to API rate limiting.\\n\")\n",
    "\n",
    "# Collect the data\n",
    "start_time = time.time()\n",
    "df_tmdb = collect_movies_for_year_range(START_YEAR, END_YEAR, pages_per_year=PAGES_PER_YEAR)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Data collection complete!\")\n",
    "print(f\"Total movies collected: {len(df_tmdb)}\")\n",
    "print(f\"Time elapsed: {(end_time - start_time) / 60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 5: Save TMDB Raw Data ==========\n",
    "# Saves collected TMDB data to CSV and displays dataset metadata.\n",
    "\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'data/raw/movies_tmdb_raw.csv'\n",
    "df_tmdb.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_tmdb)}\")\n",
    "print(f\"Total columns: {len(df_tmdb.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 6: Inspect TMDB Dataset ==========\n",
    "# Displays comprehensive EDA: shape, dtypes, missing values, statistics, and sample rows.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nShape: {df_tmdb.shape}\")\n",
    "print(f\"  Rows (movies): {df_tmdb.shape[0]}\")\n",
    "print(f\"  Columns (features): {df_tmdb.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN NAMES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = df_tmdb.isnull().sum()\n",
    "missing_pct = (missing / len(df_tmdb) * 100).round(1)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASIC STATISTICS (Numeric Columns)\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Movies with budget data: {df_tmdb['budget'].notna().sum()} ({df_tmdb['budget'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero budget: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with revenue data: {df_tmdb['revenue'].notna().sum()} ({df_tmdb['revenue'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with non-zero revenue: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with IMDb ID: {df_tmdb['imdb_id'].notna().sum()} ({df_tmdb['imdb_id'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with director: {df_tmdb['director_name'].notna().sum()} ({df_tmdb['director_name'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with cast data: {df_tmdb['cast_names'].notna().sum()} ({df_tmdb['cast_names'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "print(f\"Movies with YouTube trailer: {df_tmdb['youtube_trailer_key'].notna().sum()} ({df_tmdb['youtube_trailer_key'].notna().sum() / len(df_tmdb) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE MOVIES\")\n",
    "print(\"=\"*60)\n",
    "print(df_tmdb[['title', 'release_date', 'budget', 'revenue', 'primary_genre', 'director_name']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 7: Display TMDB Sample Rows ==========\n",
    "# Shows first 5 movies with all 27 TMDB fields for quick verification.\n",
    "\n",
    "df_tmdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 8: Load TMDB Data from CSV ==========\n",
    "# Loads existing TMDB CSV if not already in memory; supports resumable workflows.\n",
    "\n",
    "if 'df_tmdb' not in locals():\n",
    "    print(\"Loading existing TMDB data from CSV...\")\n",
    "    df_tmdb = pd.read_csv('data/raw/movies_tmdb_raw.csv')\n",
    "    print(f\"Loaded {len(df_tmdb)} movies\")\n",
    "else:\n",
    "    print(f\"df_tmdb already in memory with {len(df_tmdb)} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 9: Define BOM HTML Parsing Function ==========\n",
    "# Parses Box Office Mojo HTML to extract revenue fields (domestic, opening, worldwide, budget).\n",
    "\n",
    "def parse_bom_revenue(soup, imdb_id):\n",
    "    \"\"\"\n",
    "    Extract revenue from Box Office Mojo HTML using span.money tags.\n",
    "\n",
    "    BOM structure: Revenue values are in <span class=\"money\"> tags.\n",
    "    We find all money spans and match them to labels by proximity.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object of page HTML\n",
    "        imdb_id: IMDb ID for result dictionary\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with revenue fields or None values if not found\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': True,\n",
    "        'error_message': None\n",
    "    }\n",
    "\n",
    "    # Find all <span class=\"money\"> elements\n",
    "    money_spans = soup.find_all('span', class_='money')\n",
    "\n",
    "    # For each money span, check surrounding context for labels\n",
    "    for money_span in money_spans:\n",
    "        # Get the dollar amount\n",
    "        amount_text = money_span.get_text(strip=True)\n",
    "        if not amount_text or amount_text == '–':\n",
    "            continue\n",
    "\n",
    "        amount = int(amount_text.replace('$', '').replace(',', ''))\n",
    "\n",
    "        # Get parent div to find associated label\n",
    "        parent = money_span.find_parent('div', class_='a-section')\n",
    "        if not parent:\n",
    "            continue\n",
    "\n",
    "        # Get the text of the parent div to find label\n",
    "        parent_text = parent.get_text().lower()\n",
    "\n",
    "        # Match to appropriate field based on label in parent\n",
    "        # Use \"not result[field]\" to only capture the first occurrence\n",
    "        if 'worldwide' in parent_text and result['worldwide_total'] is None:\n",
    "            result['worldwide_total'] = amount\n",
    "        elif 'domestic' in parent_text and 'international' not in parent_text and result['domestic_total'] is None:\n",
    "            result['domestic_total'] = amount\n",
    "        elif 'international' in parent_text and result['international_total'] is None:\n",
    "            result['international_total'] = amount\n",
    "        elif 'opening' in parent_text and result['opening_weekend'] is None:\n",
    "            result['opening_weekend'] = amount\n",
    "        elif 'budget' in parent_text and result['bom_budget'] is None:\n",
    "            result['bom_budget'] = amount\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"HTML parsing function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 10: Define BOM Helper Functions ==========\n",
    "# Defines BOMRateLimiter class, error handling helpers, and currency parsing utilities.\n",
    "\n",
    "def clean_currency(currency_str):\n",
    "    \"\"\"Helper function to convert currency string to integer.\"\"\"\n",
    "    return int(currency_str.replace('$', '').replace(',', ''))\n",
    "\n",
    "def error_result(imdb_id, error_type):\n",
    "    \"\"\"Helper function to create error result dictionary.\"\"\"\n",
    "    return {\n",
    "        'imdb_id': imdb_id,\n",
    "        'domestic_total': None,\n",
    "        'opening_weekend': None,\n",
    "        'international_total': None,\n",
    "        'worldwide_total': None,\n",
    "        'bom_budget': None,\n",
    "        'scrape_success': False,\n",
    "        'error_message': error_type\n",
    "    }\n",
    "\n",
    "class BOMRateLimiter:\n",
    "    \"\"\"Rate limiter for Box Office Mojo scraping.\"\"\"\n",
    "    def __init__(self, delay=1.5):\n",
    "        self.delay = delay\n",
    "        self.last_call = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        elapsed = time.time() - self.last_call\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_call = time.time()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 11: Define Main BOM Scraping Function ==========\n",
    "# Scrapes single movie revenue from Box Office Mojo with retry logic and error handling.\n",
    "\n",
    "def scrape_bom_movie(imdb_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrape revenue data for a single movie from Box Office Mojo.\n",
    "    \n",
    "    Handles various error conditions:\n",
    "    - 404: Movie not found in BOM\n",
    "    - 429: Rate limited (exponential backoff)\n",
    "    - 5xx: Server errors (retry with delays)\n",
    "    - Timeout: Network timeout (retry once)\n",
    "    - Other exceptions: Catch and log\n",
    "    \n",
    "    Args:\n",
    "        imdb_id: IMDb ID (e.g., 'tt1375666')\n",
    "        max_retries: Maximum retry attempts for recoverable errors\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with revenue data or error information\n",
    "    \"\"\"\n",
    "    url = f\"https://www.boxofficemojo.com/title/{imdb_id}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; MovieDataCollector/1.0)'}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "            if response.status_code == 404:\n",
    "                # Movie not in Box Office Mojo database\n",
    "                return error_result(imdb_id, 'not_found')\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited - wait with exponential backoff\n",
    "                wait = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                print(f\"  Rate limited for {imdb_id}, waiting {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            elif response.status_code >= 500:\n",
    "                # Server error - retry if attempts remain\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  Server error {response.status_code} for {imdb_id}, retrying...\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                return error_result(imdb_id, f'server_error_{response.status_code}')\n",
    "\n",
    "            elif response.status_code == 200:\n",
    "                # Success - parse the HTML\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                return parse_bom_revenue(soup, imdb_id)\n",
    "\n",
    "            else:\n",
    "                # Unexpected status code\n",
    "                return error_result(imdb_id, f'http_{response.status_code}')\n",
    "\n",
    "        except requests.Timeout:\n",
    "            # Network timeout - retry if attempts remain\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  Timeout for {imdb_id}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return error_result(imdb_id, 'timeout')\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch-all for unexpected errors\n",
    "            error_msg = str(e)[:50]  # Truncate long error messages\n",
    "            return error_result(imdb_id, f'exception_{error_msg}')\n",
    "\n",
    "    # Max retries exhausted\n",
    "    return error_result(imdb_id, 'max_retries')\n",
    "\n",
    "print(\"Main scraping function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 12: Define Batch Scraping with Checkpointing ==========\n",
    "# Batch scrapes multiple movies with automatic checkpoint saving every 100 movies for resumability.\n",
    "\n",
    "def scrape_bom_batch(imdb_ids, checkpoint_file='data/raw/bom_checkpoint.csv', save_every=100):\n",
    "    \"\"\"\n",
    "    Scrape multiple movies with automatic checkpointing for resumability.\n",
    "    \n",
    "    If interrupted, the function can resume from the last checkpoint by\n",
    "    simply running again - it will load completed IMDb IDs and skip them.\n",
    "    \n",
    "    Args:\n",
    "        imdb_ids: List of IMDb IDs to scrape\n",
    "        checkpoint_file: Path to save progress (CSV format)\n",
    "        save_every: Save checkpoint every N movies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all scraped results\n",
    "    \"\"\"\n",
    "    # Load existing checkpoint if available\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "        completed = set(df_checkpoint['imdb_id'].dropna())\n",
    "        results = df_checkpoint.to_dict('records')\n",
    "        print(f\"Resuming from checkpoint: {len(completed)} already scraped\")\n",
    "    else:\n",
    "        completed = set()\n",
    "        results = []\n",
    "        print(\"Starting fresh scrape (no checkpoint found)\")\n",
    "\n",
    "    # Filter to unscraped movies\n",
    "    remaining = [id for id in imdb_ids if id not in completed]\n",
    "    print(f\"Scraping {len(remaining)} movies...\")\n",
    "    print(f\"Estimated time: {len(remaining) * 2 / 3600:.1f} hours\\n\")\n",
    "\n",
    "    # Initialize rate limiter\n",
    "    limiter = BOMRateLimiter(delay=1.5)\n",
    "\n",
    "    # Scrape each movie\n",
    "    for i, imdb_id in enumerate(remaining, 1):\n",
    "        limiter.wait()  # Respect rate limit before each request\n",
    "\n",
    "        result = scrape_bom_movie(imdb_id)\n",
    "        results.append(result)\n",
    "\n",
    "        # Progress report every 50 movies\n",
    "        if i % 50 == 0:\n",
    "            success = sum(1 for r in results[-i:] if r['scrape_success'])\n",
    "            print(f\"  Progress: {i}/{len(remaining)} | Recent success rate: {success}/{min(i, 50)} ({success/min(i, 50)*100:.1f}%)\")\n",
    "\n",
    "        # Save checkpoint every N movies\n",
    "        if i % save_every == 0:\n",
    "            pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "            print(f\"  Checkpoint saved ({len(results)} total movies)\")\n",
    "\n",
    "    # Final save\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(checkpoint_file, index=False)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    total_success = df['scrape_success'].sum()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping complete!\")\n",
    "    print(f\"  Total movies: {len(df)}\")\n",
    "    print(f\"  Successful: {total_success} ({total_success/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Failed: {len(df) - total_success}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Batch processing function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 13: Test BOM Scraper on Sample Movies ==========\n",
    "# Tests scraper on 5 sample movies to verify parsing works correctly before full run.\n",
    "#\n",
    "# Cell 19 - TEST SCRAPING ON 5 MOVIES\n",
    "# Test scraper on 5 sample movies\n",
    "print(\"Testing Box Office Mojo scraper on sample movies...\\n\")\n",
    "\n",
    "test_ids = df_tmdb.head(5)['imdb_id'].dropna().tolist()\n",
    "\n",
    "for test_id in test_ids:\n",
    "    result = scrape_bom_movie(test_id)\n",
    "    \n",
    "    # Get movie title for context\n",
    "    title = df_tmdb[df_tmdb['imdb_id'] == test_id]['title'].values[0]\n",
    "    \n",
    "    print(f\"{test_id} ({title}):\")\n",
    "    print(f\"  Success: {result['scrape_success']}\")\n",
    "    print(f\"  Worldwide: ${result['worldwide_total']:,}\" if result['worldwide_total'] else f\"  Worldwide: None\")\n",
    "    print(f\"  Domestic: ${result['domestic_total']:,}\" if result['domestic_total'] else f\"  Domestic: None\")\n",
    "    print(f\"  Opening: ${result['opening_weekend']:,}\" if result['opening_weekend'] else f\"  Opening: None\")\n",
    "    if result['error_message']:\n",
    "        print(f\"  Error: {result['error_message']}\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(1.5)  # Rate limit during test\n",
    "\n",
    "print(\"Test complete! If results look good, proceed to full scraping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 14: Execute Full BOM Scraping ==========\n",
    "# Scrapes revenue data for 5,095 movies from Box Office Mojo; resumes from checkpoint if interrupted.\n",
    "\n",
    "imdb_ids = df_tmdb['imdb_id'].dropna().tolist()\n",
    "\n",
    "print(f\"Starting Box Office Mojo scraping...\")\n",
    "print(f\"Total movies to scrape: {len(imdb_ids)}\")\n",
    "print(f\"Estimated time: {len(imdb_ids) * 2 / 3600:.1f} hours\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Run batch scraper with checkpointing\n",
    "scrape_start = time.time()\n",
    "df_bom = scrape_bom_batch(\n",
    "    imdb_ids,\n",
    "    checkpoint_file='data/raw/bom_checkpoint.csv',\n",
    "    save_every=100\n",
    ")\n",
    "scrape_end = time.time()\n",
    "\n",
    "print(f\"\\nFinished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {(scrape_end - scrape_start) / 60:.1f} minutes ({(scrape_end - scrape_start) / 3600:.2f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 15: Save BOM Raw Data ==========\n",
    "# Saves BOM records to CSV and displays file stats and sample rows.\n",
    "\n",
    "output_file = 'data/raw/revenue_boxofficemojo_raw.csv'\n",
    "df_bom.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Box Office Mojo data saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "print(f\"Total rows: {len(df_bom)}\")\n",
    "print(f\"Total columns: {len(df_bom.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_bom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 16: Merge TMDB & BOM Datasets ==========\n",
    "# Merges datasets on IMDb ID, creates consolidated budget/revenue columns, displays merge statistics.\n",
    "\n",
    "df_merged = df_tmdb.merge(\n",
    "    df_bom[['imdb_id', 'domestic_total', 'opening_weekend', 'worldwide_total', 'international_total', 'bom_budget', 'scrape_success']],\n",
    "    on='imdb_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create final revenue column - prefer BOM worldwide, fallback to TMDB revenue\n",
    "df_merged['revenue_final'] = df_merged['worldwide_total'].fillna(df_merged['revenue'])\n",
    "\n",
    "# Create final budget column - prefer BOM budget, fallback to TMDB budget\n",
    "df_merged['budget_final'] = df_merged['bom_budget'].fillna(df_merged['budget'])\n",
    "\n",
    "# Track revenue source for transparency\n",
    "df_merged['revenue_source'] = 'none'\n",
    "df_merged.loc[df_merged['revenue'] > 0, 'revenue_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['worldwide_total'].notna(), 'revenue_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna()), 'revenue_source'] = 'both'\n",
    "\n",
    "# Track budget source for transparency\n",
    "df_merged['budget_source'] = 'none'\n",
    "df_merged.loc[df_merged['budget'] > 0, 'budget_source'] = 'tmdb'\n",
    "df_merged.loc[df_merged['bom_budget'].notna(), 'budget_source'] = 'bom'\n",
    "df_merged.loc[(df_merged['budget'] > 0) & (df_merged['bom_budget'].notna()), 'budget_source'] = 'both'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MERGE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"  Rows: {len(df_merged)}\")\n",
    "print(f\"  Columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\nRevenue source breakdown:\")\n",
    "print(df_merged['revenue_source'].value_counts())\n",
    "\n",
    "print(\"\\nBudget source breakdown:\")\n",
    "print(df_merged['budget_source'].value_counts())\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "print(\"  - domestic_total (from BOM)\")\n",
    "print(\"  - opening_weekend (from BOM)\")\n",
    "print(\"  - worldwide_total (from BOM)\")\n",
    "print(\"  - international_total (from BOM)\")\n",
    "print(\"  - bom_budget (from BOM)\")\n",
    "print(\"  - revenue_final (combined best source)\")\n",
    "print(\"  - budget_final (combined best source)\")\n",
    "print(\"  - revenue_source (tracking field)\")\n",
    "print(\"  - budget_source (tracking field)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 17: Display Detailed Scraping Results ==========\n",
    "# Shows BOM scraping success rate, error breakdown, and revenue/budget coverage comparison.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BOX OFFICE MOJO SCRAPING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scraping success rate\n",
    "total_scraped = len(df_bom)\n",
    "successful = df_bom['scrape_success'].sum()\n",
    "print(f\"\\nScraping Success Rate:\")\n",
    "print(f\"  Total attempted: {total_scraped}\")\n",
    "print(f\"  Successful: {successful} ({successful/total_scraped*100:.1f}%)\")\n",
    "print(f\"  Failed: {total_scraped - successful}\")\n",
    "\n",
    "# Most common errors\n",
    "if (total_scraped - successful) > 0:\n",
    "    print(\"\\nMost common errors:\")\n",
    "    error_counts = df_bom[~df_bom['scrape_success']]['error_message'].value_counts().head(5)\n",
    "    for error, count in error_counts.items():\n",
    "        print(f\"  {error}: {count}\")\n",
    "\n",
    "# Revenue coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB revenue > 0: {(df_tmdb['revenue'] > 0).sum()} ({(df_tmdb['revenue'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM revenue available: {df_merged['worldwide_total'].notna().sum()} ({df_merged['worldwide_total'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final revenue > 0: {(df_merged['revenue_final'] > 0).sum()} ({(df_merged['revenue_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for revenue\n",
    "tmdb_missing = (df_merged['revenue'] == 0) | (df_merged['revenue'].isna())\n",
    "bom_filled = df_merged['worldwide_total'].notna()\n",
    "gaps_filled = (tmdb_missing & bom_filled).sum()\n",
    "\n",
    "print(f\"\\nRevenue Gap Filling:\")\n",
    "print(f\"  TMDB revenue gaps: {tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {gaps_filled/tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Budget coverage comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BUDGET COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  TMDB budget > 0: {(df_tmdb['budget'] > 0).sum()} ({(df_tmdb['budget'] > 0).sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  BOM budget available: {df_merged['bom_budget'].notna().sum()} ({df_merged['bom_budget'].notna().sum()/len(df_merged)*100:.1f}%)\")\n",
    "print(f\"  Final budget > 0: {(df_merged['budget_final'] > 0).sum()} ({(df_merged['budget_final'] > 0).sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n",
    "# Gap filling analysis for budget\n",
    "budget_tmdb_missing = (df_merged['budget'] == 0) | (df_merged['budget'].isna())\n",
    "budget_bom_filled = df_merged['bom_budget'].notna()\n",
    "budget_gaps_filled = (budget_tmdb_missing & budget_bom_filled).sum()\n",
    "\n",
    "print(f\"\\nBudget Gap Filling:\")\n",
    "print(f\"  TMDB budget gaps: {budget_tmdb_missing.sum()}\")\n",
    "print(f\"  Gaps filled by BOM: {budget_gaps_filled}\")\n",
    "print(f\"  Gap fill rate: {budget_gaps_filled/budget_tmdb_missing.sum()*100:.1f}%\")\n",
    "\n",
    "# Revenue comparison for movies with both sources\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"REVENUE COMPARISON (Movies with Both Sources)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "both = (df_merged['revenue'] > 0) & (df_merged['worldwide_total'].notna())\n",
    "if both.sum() > 0:\n",
    "    df_compare = df_merged[both].copy()\n",
    "    df_compare['diff'] = abs(df_compare['revenue'] - df_compare['worldwide_total'])\n",
    "    df_compare['diff_pct'] = df_compare['diff'] / df_compare['revenue'] * 100\n",
    "\n",
    "    print(f\"\\nCount: {len(df_compare)}\")\n",
    "    print(f\"Mean absolute difference: ${df_compare['diff'].mean():,.0f}\")\n",
    "    print(f\"Median absolute difference: ${df_compare['diff'].median():,.0f}\")\n",
    "    print(f\"Mean % difference: {df_compare['diff_pct'].mean():.1f}%\")\n",
    "    print(f\"Median % difference: {df_compare['diff_pct'].median():.1f}%\")\n",
    "    print(f\"\\nMovies with >20% difference: {(df_compare['diff_pct'] > 20).sum()} ({(df_compare['diff_pct'] > 20).sum()/len(df_compare)*100:.1f}%)\")\n",
    "    \n",
    "    # Show a few examples of large discrepancies\n",
    "    if (df_compare['diff_pct'] > 20).sum() > 0:\n",
    "        print(\"\\nExample large discrepancies:\")\n",
    "        large_diff = df_compare.nlargest(3, 'diff_pct')[['title', 'revenue', 'worldwide_total', 'diff_pct']]\n",
    "        for idx, row in large_diff.iterrows():\n",
    "            print(f\"  {row['title']}: TMDB=${row['revenue']:,} vs BOM=${row['worldwide_total']:,} ({row['diff_pct']:.1f}% diff)\")\n",
    "\n",
    "# Dataset readiness check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READINESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Before BOM\n",
    "complete_before = (df_tmdb['budget'] > 0) & (df_tmdb['revenue'] > 0)\n",
    "print(f\"\\nBefore BOM scraping:\")\n",
    "print(f\"  Movies with budget > 0: {(df_tmdb['budget'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue > 0: {(df_tmdb['revenue'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_before.sum()} ({complete_before.sum()/len(df_tmdb)*100:.1f}%)\")\n",
    "\n",
    "# After BOM\n",
    "complete_after = (df_merged['budget_final'] > 0) & (df_merged['revenue_final'] > 0)\n",
    "print(f\"\\nAfter BOM scraping:\")\n",
    "print(f\"  Movies with budget_final > 0: {(df_merged['budget_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with revenue_final > 0: {(df_merged['revenue_final'] > 0).sum()}\")\n",
    "print(f\"  Movies with BOTH budget & revenue: {complete_after.sum()} ({complete_after.sum()/len(df_merged)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 18: Finalize Dataset - Cleanup & Export ==========\n",
    "# Drops redundant columns, renames finals, creates placeholder columns, saves final merged CSV.\n",
    "\n",
    "\n",
    "# STEP 1: Drop redundant columns and rename final columns\n",
    "print(\"=\"*60)\n",
    "print(\"COLUMN CLEANUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nStarting columns: {len(df_merged.columns)}\")\n",
    "print(f\"Starting shape: {df_merged.shape}\")\n",
    "\n",
    "# Drop columns that have been superseded by _final versions\n",
    "# Note: 'revenue' and 'budget' from TMDB are superseded by consolidated versions\n",
    "#       'bom_budget' is superseded by budget_final\n",
    "#       'worldwide_total' is now consolidated into revenue_final\n",
    "#       'scrape_success' was only needed during collection\n",
    "columns_to_drop = ['budget', 'revenue', 'bom_budget', 'worldwide_total', 'scrape_success']\n",
    "\n",
    "print(f\"\\nDropping {len(columns_to_drop)} redundant columns:\")\n",
    "for col in columns_to_drop:\n",
    "    if col in df_merged.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(f\"  - {col} (not found, skipping)\")\n",
    "\n",
    "# Only drop columns that actually exist\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df_merged.columns]\n",
    "df_merged = df_merged.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"\\nAfter dropping: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Rename final columns for clarity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN RENAMING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rename_map = {\n",
    "    'budget_final': 'budget',\n",
    "    'revenue_final': 'revenue_worldwide'\n",
    "}\n",
    "\n",
    "print(f\"\\nRenaming {len(rename_map)} columns:\")\n",
    "for old, new in rename_map.items():\n",
    "    if old in df_merged.columns:\n",
    "        print(f\"  - {old} → {new}\")\n",
    "    else:\n",
    "        print(f\"  - {old} not found, skipping\")\n",
    "\n",
    "df_merged = df_merged.rename(columns=rename_map)\n",
    "\n",
    "# Create new empty columns for future data collection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING NEW EMPTY COLUMNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Theater data columns\n",
    "print(\"\\nTheater data columns (3):\")\n",
    "df_merged['opening_theaters'] = pd.NA\n",
    "df_merged['max_theaters'] = pd.NA\n",
    "df_merged['is_wide_release'] = pd.NA\n",
    "print(\"  - opening_theaters (int)\")\n",
    "print(\"  - max_theaters (int)\")\n",
    "print(\"  - is_wide_release (bool)\")\n",
    "\n",
    "# Franchise/sequel columns\n",
    "print(\"\\nFranchise feature columns (4):\")\n",
    "df_merged['is_sequel'] = pd.NA\n",
    "df_merged['franchise_name'] = pd.NA\n",
    "df_merged['franchise_number'] = pd.NA\n",
    "df_merged['years_since_last_installment'] = pd.NA\n",
    "print(\"  - is_sequel (bool)\")\n",
    "print(\"  - franchise_name (str)\")\n",
    "print(\"  - franchise_number (int)\")\n",
    "print(\"  - years_since_last_installment (float)\")\n",
    "\n",
    "# Trailer metrics columns\n",
    "print(\"\\nTrailer metrics columns (3):\")\n",
    "df_merged['trailer_views'] = pd.NA\n",
    "df_merged['trailer_release_date'] = pd.NA\n",
    "df_merged['days_since_trailer'] = pd.NA\n",
    "print(\"  - trailer_views (int)\")\n",
    "print(\"  - trailer_release_date (datetime)\")\n",
    "print(\"  - days_since_trailer (int)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal shape: {df_merged.shape}\")\n",
    "print(f\"Final columns: {len(df_merged.columns)}\")\n",
    "\n",
    "\n",
    "# STEP 2: Display updated column list\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPDATED COLUMN LIST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal columns: {len(df_merged.columns)}\\n\")\n",
    "\n",
    "for i, col in enumerate(df_merged.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show sample of data with key columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA (Key Financial Columns)\")\n",
    "print(\"=\"*60)\n",
    "key_cols = ['title', 'release_date', 'budget', 'revenue_worldwide', 'domestic_total', 'opening_weekend']\n",
    "print(df_merged[key_cols].head(10))\n",
    "\n",
    "# Check data completeness\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COMPLETENESS CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "completeness = {\n",
    "    'budget': (df_merged['budget'] > 0).sum(),\n",
    "    'revenue_worldwide': (df_merged['revenue_worldwide'] > 0).sum(),\n",
    "    'domestic_total': df_merged['domestic_total'].notna().sum(),\n",
    "    'opening_weekend': df_merged['opening_weekend'].notna().sum(),\n",
    "    'both_budget_revenue': ((df_merged['budget'] > 0) & (df_merged['revenue_worldwide'] > 0)).sum()\n",
    "}\n",
    "\n",
    "print(\"\\nMovies with complete data:\")\n",
    "for field, count in completeness.items():\n",
    "    pct = count / len(df_merged) * 100\n",
    "    print(f\"  {field:25s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 Ready for modeling: {completeness['both_budget_revenue']} movies have both budget & revenue\")\n",
    "\n",
    "# STEP 3: Save cleaned dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING CLEANED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save with cleaned column names\n",
    "cleaned_output = 'data/raw/movies_merged.csv'\n",
    "df_merged.to_csv(cleaned_output, index=False)\n",
    "\n",
    "print(f\"\\n✅ Cleaned dataset saved to: {cleaned_output}\")\n",
    "print(f\"   File size: {os.path.getsize(cleaned_output) / 1024:.1f} KB\")\n",
    "print(f\"   Rows: {len(df_merged):,}\")\n",
    "print(f\"   Columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext step: Proceed to 02_data_cleaning_and_eda.ipynb for data cleaning and EDA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
